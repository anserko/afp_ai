{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8ac718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f1d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737b0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d53d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from get_frames import get_frames\n",
    "from build_model_ed import build_model_1, build_model_2\n",
    "from build_model_lstm import build_model_lstm_1, build_model_lstm_2\n",
    "from create_encoder_decoder_data import process_image, process_dataset, get_full_path\n",
    "from get_encoder_decoder import get_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12eff1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#gpu check\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aac6304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (770, 80, 100)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "unit_numb = 100\n",
    "test_size = 0.1\n",
    "stride_step = 50\n",
    "norm_factor=1\n",
    "dtype = 'uint8'\n",
    "file_name = f'save_data//encoded_model_2//encoded_{unit_numb}_{stride_step}_ts_{test_size}_{dtype}_norm_{norm_factor}.pkl'\n",
    "with open(file_name, 'rb') as f:\n",
    "    frames_data_encoded = pickle.load(f)\n",
    "    \n",
    "print(f'Data shape: {frames_data_encoded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a871d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "np.random.shuffle(frames_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59f86e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_lstm: (770, 79, 100)\n",
      "y_train_lstm: (770, 79, 100)\n"
     ]
    }
   ],
   "source": [
    "#create dataset for lstm with an offset\n",
    "x_train_lstm = frames_data_encoded[:,:-1,:]\n",
    "y_train_lstm = frames_data_encoded[:,1:,:]\n",
    "print(f'x_train_lstm: {x_train_lstm.shape}')\n",
    "print(f'y_train_lstm: {y_train_lstm.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03e11d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, None, 100)]       0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, None, 200)         240800    \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, None, 100)         60100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300900 (1.15 MB)\n",
      "Trainable params: 300900 (1.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cells_list = [200]\n",
    "ifDense = False\n",
    "input_shape = (None, unit_numb)\n",
    "loss = 'mean_squared_error'\n",
    "learning_rate = 0.1\n",
    "optimizer_name = \"Adadelta\"\n",
    "optimizer = tf.keras.optimizers.legacy.Adadelta(learning_rate=learning_rate, name=optimizer_name)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "model = build_model_lstm_1(input_shape, cells_list, ifDense=ifDense, ifDropout=False)\n",
    "model.compile(loss=loss, \n",
    "              optimizer=optimizer, )\n",
    "              #metrics=metrics)\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba7e49ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.src.activations.tanh(x)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check activation function of the lstm layer\n",
    "model.layers[1].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "851991b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training 16 ifDense False bp \n",
    "\n",
    "folder = 'lstm'\n",
    "epochs = 2000\n",
    "batch_size = 2 \n",
    "validation_split = 0.1\n",
    "\n",
    "cells_list_str = '_'.join(str(x) for x in cells_list)\n",
    "\n",
    "model_name = f'model_lstm_units_{unit_numb}_bs2_{input_shape[-1]}_{cells_list_str}_ifDense_{ifDense}'\n",
    "\n",
    "#define callbacks\n",
    "# Write TensorBoard logs\n",
    "log_dir = f'./tensorboard/{folder}/logs/{model_name}'\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "#Stop training when no improvement\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                  patience=10, \n",
    "                                                  restore_best_weights=True)\n",
    "#Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=5)\n",
    "callbacks = [tensorboard, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4aedc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "if 1:\n",
    "    ld = log_dir\n",
    "else:\n",
    "    ld=None\n",
    "!rm -rf $ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7c1744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "347/347 [==============================] - 3s 5ms/step - loss: 0.2428 - val_loss: 0.2346 - lr: 0.1000\n",
      "Epoch 2/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.2295 - val_loss: 0.2221 - lr: 0.1000\n",
      "Epoch 3/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.2167 - val_loss: 0.2088 - lr: 0.1000\n",
      "Epoch 4/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.2048 - val_loss: 0.1982 - lr: 0.1000\n",
      "Epoch 5/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1957 - val_loss: 0.1902 - lr: 0.1000\n",
      "Epoch 6/2000\n",
      "347/347 [==============================] - 2s 5ms/step - loss: 0.1884 - val_loss: 0.1835 - lr: 0.1000\n",
      "Epoch 7/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1823 - val_loss: 0.1777 - lr: 0.1000\n",
      "Epoch 8/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1769 - val_loss: 0.1728 - lr: 0.1000\n",
      "Epoch 9/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1722 - val_loss: 0.1683 - lr: 0.1000\n",
      "Epoch 10/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1680 - val_loss: 0.1643 - lr: 0.1000\n",
      "Epoch 11/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1641 - val_loss: 0.1606 - lr: 0.1000\n",
      "Epoch 12/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1605 - val_loss: 0.1571 - lr: 0.1000\n",
      "Epoch 13/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1572 - val_loss: 0.1540 - lr: 0.1000\n",
      "Epoch 14/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1541 - val_loss: 0.1510 - lr: 0.1000\n",
      "Epoch 15/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1512 - val_loss: 0.1482 - lr: 0.1000\n",
      "Epoch 16/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1484 - val_loss: 0.1456 - lr: 0.1000\n",
      "Epoch 17/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1458 - val_loss: 0.1431 - lr: 0.1000\n",
      "Epoch 18/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1434 - val_loss: 0.1407 - lr: 0.1000\n",
      "Epoch 19/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1410 - val_loss: 0.1384 - lr: 0.1000\n",
      "Epoch 20/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1387 - val_loss: 0.1362 - lr: 0.1000\n",
      "Epoch 21/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1366 - val_loss: 0.1341 - lr: 0.1000\n",
      "Epoch 22/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1344 - val_loss: 0.1321 - lr: 0.1000\n",
      "Epoch 23/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1324 - val_loss: 0.1301 - lr: 0.1000\n",
      "Epoch 24/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1304 - val_loss: 0.1282 - lr: 0.1000\n",
      "Epoch 25/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1285 - val_loss: 0.1263 - lr: 0.1000\n",
      "Epoch 26/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1266 - val_loss: 0.1245 - lr: 0.1000\n",
      "Epoch 27/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1248 - val_loss: 0.1227 - lr: 0.1000\n",
      "Epoch 28/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1231 - val_loss: 0.1210 - lr: 0.1000\n",
      "Epoch 29/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1213 - val_loss: 0.1193 - lr: 0.1000\n",
      "Epoch 30/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1196 - val_loss: 0.1177 - lr: 0.1000\n",
      "Epoch 31/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1180 - val_loss: 0.1161 - lr: 0.1000\n",
      "Epoch 32/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1164 - val_loss: 0.1145 - lr: 0.1000\n",
      "Epoch 33/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1148 - val_loss: 0.1129 - lr: 0.1000\n",
      "Epoch 34/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1132 - val_loss: 0.1114 - lr: 0.1000\n",
      "Epoch 35/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1117 - val_loss: 0.1099 - lr: 0.1000\n",
      "Epoch 36/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1102 - val_loss: 0.1085 - lr: 0.1000\n",
      "Epoch 37/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1088 - val_loss: 0.1071 - lr: 0.1000\n",
      "Epoch 38/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1074 - val_loss: 0.1057 - lr: 0.1000\n",
      "Epoch 39/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1060 - val_loss: 0.1043 - lr: 0.1000\n",
      "Epoch 40/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1046 - val_loss: 0.1030 - lr: 0.1000\n",
      "Epoch 41/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1033 - val_loss: 0.1017 - lr: 0.1000\n",
      "Epoch 42/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1019 - val_loss: 0.1004 - lr: 0.1000\n",
      "Epoch 43/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.1007 - val_loss: 0.0991 - lr: 0.1000\n",
      "Epoch 44/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0994 - val_loss: 0.0979 - lr: 0.1000\n",
      "Epoch 45/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0981 - val_loss: 0.0967 - lr: 0.1000\n",
      "Epoch 46/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0969 - val_loss: 0.0955 - lr: 0.1000\n",
      "Epoch 47/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0957 - val_loss: 0.0943 - lr: 0.1000\n",
      "Epoch 48/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0946 - val_loss: 0.0932 - lr: 0.1000\n",
      "Epoch 49/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0921 - lr: 0.1000\n",
      "Epoch 50/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0923 - val_loss: 0.0910 - lr: 0.1000\n",
      "Epoch 51/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0912 - val_loss: 0.0899 - lr: 0.1000\n",
      "Epoch 52/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0901 - val_loss: 0.0889 - lr: 0.1000\n",
      "Epoch 53/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0891 - val_loss: 0.0878 - lr: 0.1000\n",
      "Epoch 54/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0880 - val_loss: 0.0868 - lr: 0.1000\n",
      "Epoch 55/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0870 - val_loss: 0.0858 - lr: 0.1000\n",
      "Epoch 56/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0860 - val_loss: 0.0849 - lr: 0.1000\n",
      "Epoch 57/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0850 - val_loss: 0.0839 - lr: 0.1000\n",
      "Epoch 58/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0841 - val_loss: 0.0830 - lr: 0.1000\n",
      "Epoch 59/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0831 - val_loss: 0.0820 - lr: 0.1000\n",
      "Epoch 60/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0822 - val_loss: 0.0811 - lr: 0.1000\n",
      "Epoch 61/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0813 - val_loss: 0.0802 - lr: 0.1000\n",
      "Epoch 62/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0804 - val_loss: 0.0794 - lr: 0.1000\n",
      "Epoch 63/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0795 - val_loss: 0.0785 - lr: 0.1000\n",
      "Epoch 64/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0777 - lr: 0.1000\n",
      "Epoch 65/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0769 - lr: 0.1000\n",
      "Epoch 66/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0770 - val_loss: 0.0760 - lr: 0.1000\n",
      "Epoch 67/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0761 - val_loss: 0.0752 - lr: 0.1000\n",
      "Epoch 68/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0753 - val_loss: 0.0745 - lr: 0.1000\n",
      "Epoch 69/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0745 - val_loss: 0.0737 - lr: 0.1000\n",
      "Epoch 70/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0738 - val_loss: 0.0729 - lr: 0.1000\n",
      "Epoch 71/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0730 - val_loss: 0.0722 - lr: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0722 - val_loss: 0.0714 - lr: 0.1000\n",
      "Epoch 73/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0715 - val_loss: 0.0707 - lr: 0.1000\n",
      "Epoch 74/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0708 - val_loss: 0.0700 - lr: 0.1000\n",
      "Epoch 75/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0701 - val_loss: 0.0693 - lr: 0.1000\n",
      "Epoch 76/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0693 - val_loss: 0.0686 - lr: 0.1000\n",
      "Epoch 77/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0686 - val_loss: 0.0679 - lr: 0.1000\n",
      "Epoch 78/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0680 - val_loss: 0.0673 - lr: 0.1000\n",
      "Epoch 79/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0673 - val_loss: 0.0666 - lr: 0.1000\n",
      "Epoch 80/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0666 - val_loss: 0.0660 - lr: 0.1000\n",
      "Epoch 81/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0660 - val_loss: 0.0653 - lr: 0.1000\n",
      "Epoch 82/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0653 - val_loss: 0.0647 - lr: 0.1000\n",
      "Epoch 83/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0647 - val_loss: 0.0640 - lr: 0.1000\n",
      "Epoch 84/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0641 - val_loss: 0.0634 - lr: 0.1000\n",
      "Epoch 85/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0634 - val_loss: 0.0628 - lr: 0.1000\n",
      "Epoch 86/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0628 - val_loss: 0.0622 - lr: 0.1000\n",
      "Epoch 87/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0622 - val_loss: 0.0616 - lr: 0.1000\n",
      "Epoch 88/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0616 - val_loss: 0.0611 - lr: 0.1000\n",
      "Epoch 89/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0610 - val_loss: 0.0605 - lr: 0.1000\n",
      "Epoch 90/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0605 - val_loss: 0.0599 - lr: 0.1000\n",
      "Epoch 91/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0599 - val_loss: 0.0594 - lr: 0.1000\n",
      "Epoch 92/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0593 - val_loss: 0.0588 - lr: 0.1000\n",
      "Epoch 93/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0588 - val_loss: 0.0583 - lr: 0.1000\n",
      "Epoch 94/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0582 - val_loss: 0.0577 - lr: 0.1000\n",
      "Epoch 95/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0577 - val_loss: 0.0572 - lr: 0.1000\n",
      "Epoch 96/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0572 - val_loss: 0.0567 - lr: 0.1000\n",
      "Epoch 97/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0566 - val_loss: 0.0562 - lr: 0.1000\n",
      "Epoch 98/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0561 - val_loss: 0.0557 - lr: 0.1000\n",
      "Epoch 99/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0556 - val_loss: 0.0552 - lr: 0.1000\n",
      "Epoch 100/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0551 - val_loss: 0.0547 - lr: 0.1000\n",
      "Epoch 101/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0546 - val_loss: 0.0542 - lr: 0.1000\n",
      "Epoch 102/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0541 - val_loss: 0.0537 - lr: 0.1000\n",
      "Epoch 103/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0536 - val_loss: 0.0532 - lr: 0.1000\n",
      "Epoch 104/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0531 - val_loss: 0.0527 - lr: 0.1000\n",
      "Epoch 105/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0526 - val_loss: 0.0523 - lr: 0.1000\n",
      "Epoch 106/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0522 - val_loss: 0.0518 - lr: 0.1000\n",
      "Epoch 107/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0517 - val_loss: 0.0514 - lr: 0.1000\n",
      "Epoch 108/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0513 - val_loss: 0.0509 - lr: 0.1000\n",
      "Epoch 109/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0508 - val_loss: 0.0505 - lr: 0.1000\n",
      "Epoch 110/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0503 - val_loss: 0.0500 - lr: 0.1000\n",
      "Epoch 111/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0499 - val_loss: 0.0496 - lr: 0.1000\n",
      "Epoch 112/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0495 - val_loss: 0.0492 - lr: 0.1000\n",
      "Epoch 113/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0490 - val_loss: 0.0487 - lr: 0.1000\n",
      "Epoch 114/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0486 - val_loss: 0.0483 - lr: 0.1000\n",
      "Epoch 115/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0482 - val_loss: 0.0479 - lr: 0.1000\n",
      "Epoch 116/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0478 - val_loss: 0.0475 - lr: 0.1000\n",
      "Epoch 117/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0474 - val_loss: 0.0471 - lr: 0.1000\n",
      "Epoch 118/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0469 - val_loss: 0.0467 - lr: 0.1000\n",
      "Epoch 119/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0465 - val_loss: 0.0463 - lr: 0.1000\n",
      "Epoch 120/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0461 - val_loss: 0.0459 - lr: 0.1000\n",
      "Epoch 121/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0458 - val_loss: 0.0455 - lr: 0.1000\n",
      "Epoch 122/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0454 - val_loss: 0.0451 - lr: 0.1000\n",
      "Epoch 123/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0450 - val_loss: 0.0448 - lr: 0.1000\n",
      "Epoch 124/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0446 - val_loss: 0.0444 - lr: 0.1000\n",
      "Epoch 125/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0442 - val_loss: 0.0440 - lr: 0.1000\n",
      "Epoch 126/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0439 - val_loss: 0.0436 - lr: 0.1000\n",
      "Epoch 127/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0435 - val_loss: 0.0433 - lr: 0.1000\n",
      "Epoch 128/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0431 - val_loss: 0.0429 - lr: 0.1000\n",
      "Epoch 129/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0428 - val_loss: 0.0426 - lr: 0.1000\n",
      "Epoch 130/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0424 - val_loss: 0.0422 - lr: 0.1000\n",
      "Epoch 131/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0421 - val_loss: 0.0419 - lr: 0.1000\n",
      "Epoch 132/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0417 - val_loss: 0.0415 - lr: 0.1000\n",
      "Epoch 133/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0414 - val_loss: 0.0412 - lr: 0.1000\n",
      "Epoch 134/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0410 - val_loss: 0.0409 - lr: 0.1000\n",
      "Epoch 135/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0407 - val_loss: 0.0405 - lr: 0.1000\n",
      "Epoch 136/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0404 - val_loss: 0.0402 - lr: 0.1000\n",
      "Epoch 137/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0400 - val_loss: 0.0399 - lr: 0.1000\n",
      "Epoch 138/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0397 - val_loss: 0.0396 - lr: 0.1000\n",
      "Epoch 139/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0394 - val_loss: 0.0392 - lr: 0.1000\n",
      "Epoch 140/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0391 - val_loss: 0.0389 - lr: 0.1000\n",
      "Epoch 141/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0387 - val_loss: 0.0386 - lr: 0.1000\n",
      "Epoch 142/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0384 - val_loss: 0.0383 - lr: 0.1000\n",
      "Epoch 143/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0381 - val_loss: 0.0380 - lr: 0.1000\n",
      "Epoch 144/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0378 - val_loss: 0.0377 - lr: 0.1000\n",
      "Epoch 145/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0375 - val_loss: 0.0374 - lr: 0.1000\n",
      "Epoch 146/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0372 - val_loss: 0.0371 - lr: 0.1000\n",
      "Epoch 147/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0369 - val_loss: 0.0368 - lr: 0.1000\n",
      "Epoch 148/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0366 - val_loss: 0.0365 - lr: 0.1000\n",
      "Epoch 149/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0364 - val_loss: 0.0363 - lr: 0.1000\n",
      "Epoch 150/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0361 - val_loss: 0.0360 - lr: 0.1000\n",
      "Epoch 151/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0358 - val_loss: 0.0357 - lr: 0.1000\n",
      "Epoch 152/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0355 - val_loss: 0.0354 - lr: 0.1000\n",
      "Epoch 153/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0352 - val_loss: 0.0352 - lr: 0.1000\n",
      "Epoch 154/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0350 - val_loss: 0.0349 - lr: 0.1000\n",
      "Epoch 155/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0347 - val_loss: 0.0346 - lr: 0.1000\n",
      "Epoch 156/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0344 - val_loss: 0.0344 - lr: 0.1000\n",
      "Epoch 157/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0342 - val_loss: 0.0341 - lr: 0.1000\n",
      "Epoch 158/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0339 - val_loss: 0.0338 - lr: 0.1000\n",
      "Epoch 159/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0337 - val_loss: 0.0336 - lr: 0.1000\n",
      "Epoch 160/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0334 - val_loss: 0.0333 - lr: 0.1000\n",
      "Epoch 161/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0331 - val_loss: 0.0331 - lr: 0.1000\n",
      "Epoch 162/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0329 - val_loss: 0.0328 - lr: 0.1000\n",
      "Epoch 163/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0327 - val_loss: 0.0326 - lr: 0.1000\n",
      "Epoch 164/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0324 - val_loss: 0.0324 - lr: 0.1000\n",
      "Epoch 165/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0322 - val_loss: 0.0321 - lr: 0.1000\n",
      "Epoch 166/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0319 - val_loss: 0.0319 - lr: 0.1000\n",
      "Epoch 167/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0317 - val_loss: 0.0317 - lr: 0.1000\n",
      "Epoch 168/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0315 - val_loss: 0.0314 - lr: 0.1000\n",
      "Epoch 169/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0312 - val_loss: 0.0312 - lr: 0.1000\n",
      "Epoch 170/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0310 - val_loss: 0.0310 - lr: 0.1000\n",
      "Epoch 171/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0308 - val_loss: 0.0308 - lr: 0.1000\n",
      "Epoch 172/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0306 - val_loss: 0.0305 - lr: 0.1000\n",
      "Epoch 173/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0303 - val_loss: 0.0303 - lr: 0.1000\n",
      "Epoch 174/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0301 - val_loss: 0.0301 - lr: 0.1000\n",
      "Epoch 175/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0299 - val_loss: 0.0299 - lr: 0.1000\n",
      "Epoch 176/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0297 - val_loss: 0.0297 - lr: 0.1000\n",
      "Epoch 177/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0295 - val_loss: 0.0295 - lr: 0.1000\n",
      "Epoch 178/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0293 - val_loss: 0.0293 - lr: 0.1000\n",
      "Epoch 179/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0291 - val_loss: 0.0290 - lr: 0.1000\n",
      "Epoch 180/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0288 - val_loss: 0.0288 - lr: 0.1000\n",
      "Epoch 181/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0286 - val_loss: 0.0286 - lr: 0.1000\n",
      "Epoch 182/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0284 - val_loss: 0.0284 - lr: 0.1000\n",
      "Epoch 183/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0282 - val_loss: 0.0282 - lr: 0.1000\n",
      "Epoch 184/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0280 - val_loss: 0.0281 - lr: 0.1000\n",
      "Epoch 185/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0279 - val_loss: 0.0279 - lr: 0.1000\n",
      "Epoch 186/2000\n",
      "347/347 [==============================] - 2s 5ms/step - loss: 0.0277 - val_loss: 0.0277 - lr: 0.1000\n",
      "Epoch 187/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0275 - val_loss: 0.0275 - lr: 0.1000\n",
      "Epoch 188/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0273 - val_loss: 0.0273 - lr: 0.1000\n",
      "Epoch 189/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0271 - val_loss: 0.0271 - lr: 0.1000\n",
      "Epoch 190/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0269 - val_loss: 0.0269 - lr: 0.1000\n",
      "Epoch 191/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0267 - val_loss: 0.0267 - lr: 0.1000\n",
      "Epoch 192/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0265 - val_loss: 0.0266 - lr: 0.1000\n",
      "Epoch 193/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0264 - val_loss: 0.0264 - lr: 0.1000\n",
      "Epoch 194/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0262 - val_loss: 0.0262 - lr: 0.1000\n",
      "Epoch 195/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0260 - val_loss: 0.0260 - lr: 0.1000\n",
      "Epoch 196/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0258 - val_loss: 0.0259 - lr: 0.1000\n",
      "Epoch 197/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0257 - val_loss: 0.0257 - lr: 0.1000\n",
      "Epoch 198/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0255 - val_loss: 0.0255 - lr: 0.1000\n",
      "Epoch 199/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0253 - val_loss: 0.0254 - lr: 0.1000\n",
      "Epoch 200/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0252 - val_loss: 0.0252 - lr: 0.1000\n",
      "Epoch 201/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0250 - val_loss: 0.0250 - lr: 0.1000\n",
      "Epoch 202/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0248 - val_loss: 0.0249 - lr: 0.1000\n",
      "Epoch 203/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0247 - val_loss: 0.0247 - lr: 0.1000\n",
      "Epoch 204/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0245 - val_loss: 0.0246 - lr: 0.1000\n",
      "Epoch 205/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0244 - val_loss: 0.0244 - lr: 0.1000\n",
      "Epoch 206/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0242 - val_loss: 0.0242 - lr: 0.1000\n",
      "Epoch 207/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0240 - val_loss: 0.0241 - lr: 0.1000\n",
      "Epoch 208/2000\n",
      "347/347 [==============================] - 2s 4ms/step - loss: 0.0239 - val_loss: 0.0239 - lr: 0.1000\n",
      "Epoch 209/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0237 - val_loss: 0.0238 - lr: 0.1000\n",
      "Epoch 210/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0236 - val_loss: 0.0236 - lr: 0.1000\n",
      "Epoch 211/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0234 - val_loss: 0.0235 - lr: 0.1000\n",
      "Epoch 212/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0233 - val_loss: 0.0233 - lr: 0.1000\n",
      "Epoch 213/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0231 - val_loss: 0.0232 - lr: 0.1000\n",
      "Epoch 214/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0230 - val_loss: 0.0231 - lr: 0.1000\n",
      "Epoch 215/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0229 - val_loss: 0.0229 - lr: 0.1000\n",
      "Epoch 216/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0227 - val_loss: 0.0228 - lr: 0.1000\n",
      "Epoch 217/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0226 - val_loss: 0.0226 - lr: 0.1000\n",
      "Epoch 218/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0224 - val_loss: 0.0225 - lr: 0.1000\n",
      "Epoch 219/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0223 - val_loss: 0.0224 - lr: 0.1000\n",
      "Epoch 220/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0222 - val_loss: 0.0222 - lr: 0.1000\n",
      "Epoch 221/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0220 - val_loss: 0.0221 - lr: 0.1000\n",
      "Epoch 222/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0219 - val_loss: 0.0220 - lr: 0.1000\n",
      "Epoch 223/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0218 - val_loss: 0.0218 - lr: 0.1000\n",
      "Epoch 224/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0216 - val_loss: 0.0217 - lr: 0.1000\n",
      "Epoch 225/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0215 - val_loss: 0.0216 - lr: 0.1000\n",
      "Epoch 226/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0214 - val_loss: 0.0214 - lr: 0.1000\n",
      "Epoch 227/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0212 - val_loss: 0.0213 - lr: 0.1000\n",
      "Epoch 228/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0211 - val_loss: 0.0212 - lr: 0.1000\n",
      "Epoch 229/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0210 - val_loss: 0.0211 - lr: 0.1000\n",
      "Epoch 230/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0209 - val_loss: 0.0209 - lr: 0.1000\n",
      "Epoch 231/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0207 - val_loss: 0.0208 - lr: 0.1000\n",
      "Epoch 232/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0206 - val_loss: 0.0207 - lr: 0.1000\n",
      "Epoch 233/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0205 - val_loss: 0.0206 - lr: 0.1000\n",
      "Epoch 234/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0204 - val_loss: 0.0205 - lr: 0.1000\n",
      "Epoch 235/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0203 - val_loss: 0.0203 - lr: 0.1000\n",
      "Epoch 236/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0201 - val_loss: 0.0202 - lr: 0.1000\n",
      "Epoch 237/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0200 - val_loss: 0.0201 - lr: 0.1000\n",
      "Epoch 238/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0199 - val_loss: 0.0200 - lr: 0.1000\n",
      "Epoch 239/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0198 - val_loss: 0.0199 - lr: 0.1000\n",
      "Epoch 240/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0197 - val_loss: 0.0198 - lr: 0.1000\n",
      "Epoch 241/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0196 - val_loss: 0.0197 - lr: 0.1000\n",
      "Epoch 242/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0195 - val_loss: 0.0195 - lr: 0.1000\n",
      "Epoch 243/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0193 - val_loss: 0.0194 - lr: 0.1000\n",
      "Epoch 244/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0192 - val_loss: 0.0193 - lr: 0.1000\n",
      "Epoch 245/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0191 - val_loss: 0.0192 - lr: 0.1000\n",
      "Epoch 246/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0190 - val_loss: 0.0191 - lr: 0.1000\n",
      "Epoch 247/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0189 - val_loss: 0.0190 - lr: 0.1000\n",
      "Epoch 248/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0188 - val_loss: 0.0189 - lr: 0.1000\n",
      "Epoch 249/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0187 - val_loss: 0.0188 - lr: 0.1000\n",
      "Epoch 250/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0186 - val_loss: 0.0187 - lr: 0.1000\n",
      "Epoch 251/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0185 - val_loss: 0.0186 - lr: 0.1000\n",
      "Epoch 252/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0184 - val_loss: 0.0185 - lr: 0.1000\n",
      "Epoch 253/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0183 - val_loss: 0.0184 - lr: 0.1000\n",
      "Epoch 254/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0182 - val_loss: 0.0183 - lr: 0.1000\n",
      "Epoch 255/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0181 - val_loss: 0.0182 - lr: 0.1000\n",
      "Epoch 256/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0180 - val_loss: 0.0181 - lr: 0.1000\n",
      "Epoch 257/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0179 - val_loss: 0.0180 - lr: 0.1000\n",
      "Epoch 258/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0178 - val_loss: 0.0179 - lr: 0.1000\n",
      "Epoch 259/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0177 - val_loss: 0.0178 - lr: 0.1000\n",
      "Epoch 260/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0176 - val_loss: 0.0177 - lr: 0.1000\n",
      "Epoch 261/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0175 - val_loss: 0.0176 - lr: 0.1000\n",
      "Epoch 262/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0174 - val_loss: 0.0175 - lr: 0.1000\n",
      "Epoch 263/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0173 - val_loss: 0.0174 - lr: 0.1000\n",
      "Epoch 264/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0173 - val_loss: 0.0174 - lr: 0.1000\n",
      "Epoch 265/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0172 - val_loss: 0.0173 - lr: 0.1000\n",
      "Epoch 266/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0171 - val_loss: 0.0172 - lr: 0.1000\n",
      "Epoch 267/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0170 - val_loss: 0.0171 - lr: 0.1000\n",
      "Epoch 268/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0169 - val_loss: 0.0170 - lr: 0.1000\n",
      "Epoch 269/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0168 - val_loss: 0.0169 - lr: 0.1000\n",
      "Epoch 270/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0167 - val_loss: 0.0168 - lr: 0.1000\n",
      "Epoch 271/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0166 - val_loss: 0.0167 - lr: 0.1000\n",
      "Epoch 272/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0166 - val_loss: 0.0167 - lr: 0.1000\n",
      "Epoch 273/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0165 - val_loss: 0.0166 - lr: 0.1000\n",
      "Epoch 274/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0164 - val_loss: 0.0165 - lr: 0.1000\n",
      "Epoch 275/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0163 - val_loss: 0.0164 - lr: 0.1000\n",
      "Epoch 276/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0162 - val_loss: 0.0163 - lr: 0.1000\n",
      "Epoch 277/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0161 - val_loss: 0.0162 - lr: 0.1000\n",
      "Epoch 278/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0161 - val_loss: 0.0162 - lr: 0.1000\n",
      "Epoch 279/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0160 - val_loss: 0.0161 - lr: 0.1000\n",
      "Epoch 280/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0159 - val_loss: 0.0160 - lr: 0.1000\n",
      "Epoch 281/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0158 - val_loss: 0.0159 - lr: 0.1000\n",
      "Epoch 282/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0157 - val_loss: 0.0158 - lr: 0.1000\n",
      "Epoch 283/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0157 - val_loss: 0.0158 - lr: 0.1000\n",
      "Epoch 284/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0156 - val_loss: 0.0157 - lr: 0.1000\n",
      "Epoch 285/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0155 - val_loss: 0.0156 - lr: 0.1000\n",
      "Epoch 286/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0154 - val_loss: 0.0155 - lr: 0.1000\n",
      "Epoch 287/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0154 - val_loss: 0.0155 - lr: 0.1000\n",
      "Epoch 288/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0153 - val_loss: 0.0154 - lr: 0.1000\n",
      "Epoch 289/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0152 - val_loss: 0.0153 - lr: 0.1000\n",
      "Epoch 290/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0151 - val_loss: 0.0152 - lr: 0.1000\n",
      "Epoch 291/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0151 - val_loss: 0.0152 - lr: 0.1000\n",
      "Epoch 292/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0150 - val_loss: 0.0151 - lr: 0.1000\n",
      "Epoch 293/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0149 - val_loss: 0.0150 - lr: 0.1000\n",
      "Epoch 294/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0148 - val_loss: 0.0150 - lr: 0.1000\n",
      "Epoch 295/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0148 - val_loss: 0.0149 - lr: 0.1000\n",
      "Epoch 296/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0147 - val_loss: 0.0148 - lr: 0.1000\n",
      "Epoch 297/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0146 - val_loss: 0.0148 - lr: 0.1000\n",
      "Epoch 298/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0146 - val_loss: 0.0147 - lr: 0.1000\n",
      "Epoch 299/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0145 - val_loss: 0.0146 - lr: 0.1000\n",
      "Epoch 300/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0144 - val_loss: 0.0145 - lr: 0.1000\n",
      "Epoch 301/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0144 - val_loss: 0.0145 - lr: 0.1000\n",
      "Epoch 302/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0143 - val_loss: 0.0144 - lr: 0.1000\n",
      "Epoch 303/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0142 - val_loss: 0.0144 - lr: 0.1000\n",
      "Epoch 304/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0142 - val_loss: 0.0143 - lr: 0.1000\n",
      "Epoch 305/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0141 - val_loss: 0.0142 - lr: 0.1000\n",
      "Epoch 306/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0140 - val_loss: 0.0142 - lr: 0.1000\n",
      "Epoch 307/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0140 - val_loss: 0.0141 - lr: 0.1000\n",
      "Epoch 308/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0139 - val_loss: 0.0140 - lr: 0.1000\n",
      "Epoch 309/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0138 - val_loss: 0.0140 - lr: 0.1000\n",
      "Epoch 310/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0138 - val_loss: 0.0139 - lr: 0.1000\n",
      "Epoch 311/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0137 - val_loss: 0.0138 - lr: 0.1000\n",
      "Epoch 312/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0137 - val_loss: 0.0138 - lr: 0.1000\n",
      "Epoch 313/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0136 - val_loss: 0.0137 - lr: 0.1000\n",
      "Epoch 314/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0135 - val_loss: 0.0137 - lr: 0.1000\n",
      "Epoch 315/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0135 - val_loss: 0.0136 - lr: 0.1000\n",
      "Epoch 316/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0134 - val_loss: 0.0135 - lr: 0.1000\n",
      "Epoch 317/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0134 - val_loss: 0.0135 - lr: 0.1000\n",
      "Epoch 318/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0133 - val_loss: 0.0134 - lr: 0.1000\n",
      "Epoch 319/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0132 - val_loss: 0.0134 - lr: 0.1000\n",
      "Epoch 320/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0132 - val_loss: 0.0133 - lr: 0.1000\n",
      "Epoch 321/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0131 - val_loss: 0.0133 - lr: 0.1000\n",
      "Epoch 322/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0131 - val_loss: 0.0132 - lr: 0.1000\n",
      "Epoch 323/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0130 - val_loss: 0.0131 - lr: 0.1000\n",
      "Epoch 324/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0130 - val_loss: 0.0131 - lr: 0.1000\n",
      "Epoch 325/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0129 - val_loss: 0.0130 - lr: 0.1000\n",
      "Epoch 326/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0128 - val_loss: 0.0130 - lr: 0.1000\n",
      "Epoch 327/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0128 - val_loss: 0.0129 - lr: 0.1000\n",
      "Epoch 328/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0127 - val_loss: 0.0129 - lr: 0.1000\n",
      "Epoch 329/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0127 - val_loss: 0.0128 - lr: 0.1000\n",
      "Epoch 330/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0126 - val_loss: 0.0128 - lr: 0.1000\n",
      "Epoch 331/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0126 - val_loss: 0.0127 - lr: 0.1000\n",
      "Epoch 332/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0125 - val_loss: 0.0126 - lr: 0.1000\n",
      "Epoch 333/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0125 - val_loss: 0.0126 - lr: 0.1000\n",
      "Epoch 334/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0124 - val_loss: 0.0125 - lr: 0.1000\n",
      "Epoch 335/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0124 - val_loss: 0.0125 - lr: 0.1000\n",
      "Epoch 336/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0123 - val_loss: 0.0124 - lr: 0.1000\n",
      "Epoch 337/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0123 - val_loss: 0.0124 - lr: 0.1000\n",
      "Epoch 338/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0122 - val_loss: 0.0123 - lr: 0.1000\n",
      "Epoch 339/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0122 - val_loss: 0.0123 - lr: 0.1000\n",
      "Epoch 340/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0121 - val_loss: 0.0122 - lr: 0.1000\n",
      "Epoch 341/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0121 - val_loss: 0.0122 - lr: 0.1000\n",
      "Epoch 342/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0120 - val_loss: 0.0121 - lr: 0.1000\n",
      "Epoch 343/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0120 - val_loss: 0.0121 - lr: 0.1000\n",
      "Epoch 344/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0119 - val_loss: 0.0120 - lr: 0.1000\n",
      "Epoch 345/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0119 - val_loss: 0.0120 - lr: 0.1000\n",
      "Epoch 346/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0118 - val_loss: 0.0119 - lr: 0.1000\n",
      "Epoch 347/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0118 - val_loss: 0.0119 - lr: 0.1000\n",
      "Epoch 348/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0117 - val_loss: 0.0119 - lr: 0.1000\n",
      "Epoch 349/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0117 - val_loss: 0.0118 - lr: 0.1000\n",
      "Epoch 350/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0116 - val_loss: 0.0118 - lr: 0.1000\n",
      "Epoch 351/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0116 - val_loss: 0.0117 - lr: 0.1000\n",
      "Epoch 352/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0115 - val_loss: 0.0117 - lr: 0.1000\n",
      "Epoch 353/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0115 - val_loss: 0.0116 - lr: 0.1000\n",
      "Epoch 354/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0115 - val_loss: 0.0116 - lr: 0.1000\n",
      "Epoch 355/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0115 - lr: 0.1000\n",
      "Epoch 356/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0114 - val_loss: 0.0115 - lr: 0.1000\n",
      "Epoch 357/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0113 - val_loss: 0.0114 - lr: 0.1000\n",
      "Epoch 358/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0113 - val_loss: 0.0114 - lr: 0.1000\n",
      "Epoch 359/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0112 - val_loss: 0.0114 - lr: 0.1000\n",
      "Epoch 360/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0112 - val_loss: 0.0113 - lr: 0.1000\n",
      "Epoch 361/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0111 - val_loss: 0.0113 - lr: 0.1000\n",
      "Epoch 362/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0111 - val_loss: 0.0112 - lr: 0.1000\n",
      "Epoch 363/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0111 - val_loss: 0.0112 - lr: 0.1000\n",
      "Epoch 364/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0111 - lr: 0.1000\n",
      "Epoch 365/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0110 - val_loss: 0.0111 - lr: 0.1000\n",
      "Epoch 366/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0109 - val_loss: 0.0111 - lr: 0.1000\n",
      "Epoch 367/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0109 - val_loss: 0.0110 - lr: 0.1000\n",
      "Epoch 368/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0108 - val_loss: 0.0110 - lr: 0.1000\n",
      "Epoch 369/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0108 - val_loss: 0.0109 - lr: 0.1000\n",
      "Epoch 370/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0108 - val_loss: 0.0109 - lr: 0.1000\n",
      "Epoch 371/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0107 - val_loss: 0.0109 - lr: 0.1000\n",
      "Epoch 372/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0107 - val_loss: 0.0108 - lr: 0.1000\n",
      "Epoch 373/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0106 - val_loss: 0.0108 - lr: 0.1000\n",
      "Epoch 374/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0106 - val_loss: 0.0107 - lr: 0.1000\n",
      "Epoch 375/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0106 - val_loss: 0.0107 - lr: 0.1000\n",
      "Epoch 376/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0105 - val_loss: 0.0107 - lr: 0.1000\n",
      "Epoch 377/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0105 - val_loss: 0.0106 - lr: 0.1000\n",
      "Epoch 378/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0105 - val_loss: 0.0106 - lr: 0.1000\n",
      "Epoch 379/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0104 - val_loss: 0.0105 - lr: 0.1000\n",
      "Epoch 380/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0104 - val_loss: 0.0105 - lr: 0.1000\n",
      "Epoch 381/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0103 - val_loss: 0.0105 - lr: 0.1000\n",
      "Epoch 382/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0103 - val_loss: 0.0104 - lr: 0.1000\n",
      "Epoch 383/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0103 - val_loss: 0.0104 - lr: 0.1000\n",
      "Epoch 384/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0102 - val_loss: 0.0104 - lr: 0.1000\n",
      "Epoch 385/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0102 - val_loss: 0.0103 - lr: 0.1000\n",
      "Epoch 386/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0102 - val_loss: 0.0103 - lr: 0.1000\n",
      "Epoch 387/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0101 - val_loss: 0.0103 - lr: 0.1000\n",
      "Epoch 388/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0101 - val_loss: 0.0102 - lr: 0.1000\n",
      "Epoch 389/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0100 - val_loss: 0.0102 - lr: 0.1000\n",
      "Epoch 390/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0100 - val_loss: 0.0101 - lr: 0.1000\n",
      "Epoch 391/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0100 - val_loss: 0.0101 - lr: 0.1000\n",
      "Epoch 392/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0099 - val_loss: 0.0101 - lr: 0.1000\n",
      "Epoch 393/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0099 - val_loss: 0.0100 - lr: 0.1000\n",
      "Epoch 394/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0099 - val_loss: 0.0100 - lr: 0.1000\n",
      "Epoch 395/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0098 - val_loss: 0.0100 - lr: 0.1000\n",
      "Epoch 396/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0098 - val_loss: 0.0099 - lr: 0.1000\n",
      "Epoch 397/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0098 - val_loss: 0.0099 - lr: 0.1000\n",
      "Epoch 398/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0097 - val_loss: 0.0099 - lr: 0.1000\n",
      "Epoch 399/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0097 - val_loss: 0.0098 - lr: 0.1000\n",
      "Epoch 400/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0097 - val_loss: 0.0098 - lr: 0.1000\n",
      "Epoch 401/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0098 - lr: 0.1000\n",
      "Epoch 402/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0097 - lr: 0.1000\n",
      "Epoch 403/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0096 - val_loss: 0.0097 - lr: 0.1000\n",
      "Epoch 404/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0095 - val_loss: 0.0097 - lr: 0.1000\n",
      "Epoch 405/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0095 - val_loss: 0.0096 - lr: 0.1000\n",
      "Epoch 406/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0095 - val_loss: 0.0096 - lr: 0.1000\n",
      "Epoch 407/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0094 - val_loss: 0.0096 - lr: 0.1000\n",
      "Epoch 408/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0094 - val_loss: 0.0095 - lr: 0.1000\n",
      "Epoch 409/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0094 - val_loss: 0.0095 - lr: 0.1000\n",
      "Epoch 410/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0093 - val_loss: 0.0095 - lr: 0.1000\n",
      "Epoch 411/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0093 - val_loss: 0.0095 - lr: 0.1000\n",
      "Epoch 412/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0093 - val_loss: 0.0094 - lr: 0.1000\n",
      "Epoch 413/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0093 - val_loss: 0.0094 - lr: 0.1000\n",
      "Epoch 414/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0092 - val_loss: 0.0094 - lr: 0.1000\n",
      "Epoch 415/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0092 - val_loss: 0.0093 - lr: 0.1000\n",
      "Epoch 416/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0092 - val_loss: 0.0093 - lr: 0.1000\n",
      "Epoch 417/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0091 - val_loss: 0.0093 - lr: 0.1000\n",
      "Epoch 418/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0091 - val_loss: 0.0092 - lr: 0.1000\n",
      "Epoch 419/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0091 - val_loss: 0.0092 - lr: 0.1000\n",
      "Epoch 420/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0090 - val_loss: 0.0092 - lr: 0.1000\n",
      "Epoch 421/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0090 - val_loss: 0.0092 - lr: 0.1000\n",
      "Epoch 422/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0090 - val_loss: 0.0091 - lr: 0.1000\n",
      "Epoch 423/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0090 - val_loss: 0.0091 - lr: 0.1000\n",
      "Epoch 424/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0089 - val_loss: 0.0091 - lr: 0.1000\n",
      "Epoch 425/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0089 - val_loss: 0.0090 - lr: 0.1000\n",
      "Epoch 426/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0089 - val_loss: 0.0090 - lr: 0.1000\n",
      "Epoch 427/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0088 - val_loss: 0.0090 - lr: 0.1000\n",
      "Epoch 428/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0088 - val_loss: 0.0090 - lr: 0.1000\n",
      "Epoch 429/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0088 - val_loss: 0.0089 - lr: 0.1000\n",
      "Epoch 430/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0088 - val_loss: 0.0089 - lr: 0.1000\n",
      "Epoch 431/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0087 - val_loss: 0.0089 - lr: 0.1000\n",
      "Epoch 432/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0087 - val_loss: 0.0088 - lr: 0.1000\n",
      "Epoch 433/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0087 - val_loss: 0.0088 - lr: 0.1000\n",
      "Epoch 434/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0088 - lr: 0.1000\n",
      "Epoch 435/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0088 - lr: 0.1000\n",
      "Epoch 436/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0087 - lr: 0.1000\n",
      "Epoch 437/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0087 - lr: 0.1000\n",
      "Epoch 438/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0087 - lr: 0.1000\n",
      "Epoch 439/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0087 - lr: 0.1000\n",
      "Epoch 440/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0086 - lr: 0.1000\n",
      "Epoch 441/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0086 - lr: 0.1000\n",
      "Epoch 442/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0084 - val_loss: 0.0086 - lr: 0.1000\n",
      "Epoch 443/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0084 - val_loss: 0.0086 - lr: 0.1000\n",
      "Epoch 444/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0084 - val_loss: 0.0085 - lr: 0.1000\n",
      "Epoch 445/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0084 - val_loss: 0.0085 - lr: 0.1000\n",
      "Epoch 446/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0085 - lr: 0.1000\n",
      "Epoch 447/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0085 - lr: 0.1000\n",
      "Epoch 448/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0084 - lr: 0.1000\n",
      "Epoch 449/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0084 - lr: 0.1000\n",
      "Epoch 450/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0082 - val_loss: 0.0084 - lr: 0.1000\n",
      "Epoch 451/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0082 - val_loss: 0.0084 - lr: 0.1000\n",
      "Epoch 452/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0082 - val_loss: 0.0083 - lr: 0.1000\n",
      "Epoch 453/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0082 - val_loss: 0.0083 - lr: 0.1000\n",
      "Epoch 454/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0081 - val_loss: 0.0083 - lr: 0.1000\n",
      "Epoch 455/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0081 - val_loss: 0.0083 - lr: 0.1000\n",
      "Epoch 456/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0081 - val_loss: 0.0082 - lr: 0.1000\n",
      "Epoch 457/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0081 - val_loss: 0.0082 - lr: 0.1000\n",
      "Epoch 458/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.0082 - lr: 0.1000\n",
      "Epoch 459/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.0082 - lr: 0.1000\n",
      "Epoch 460/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.0081 - lr: 0.1000\n",
      "Epoch 461/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.0081 - lr: 0.1000\n",
      "Epoch 462/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0081 - lr: 0.1000\n",
      "Epoch 463/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0081 - lr: 0.1000\n",
      "Epoch 464/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0080 - lr: 0.1000\n",
      "Epoch 465/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0080 - lr: 0.1000\n",
      "Epoch 466/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0080 - lr: 0.1000\n",
      "Epoch 467/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0080 - lr: 0.1000\n",
      "Epoch 468/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0080 - lr: 0.1000\n",
      "Epoch 469/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0079 - lr: 0.1000\n",
      "Epoch 470/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0079 - lr: 0.1000\n",
      "Epoch 471/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0079 - lr: 0.1000\n",
      "Epoch 472/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0079 - lr: 0.1000\n",
      "Epoch 473/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0078 - lr: 0.1000\n",
      "Epoch 474/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0078 - lr: 0.1000\n",
      "Epoch 475/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0077 - val_loss: 0.0078 - lr: 0.1000\n",
      "Epoch 476/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0078 - lr: 0.1000\n",
      "Epoch 477/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0078 - lr: 0.1000\n",
      "Epoch 478/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0077 - lr: 0.1000\n",
      "Epoch 479/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0077 - lr: 0.1000\n",
      "Epoch 480/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0077 - lr: 0.1000\n",
      "Epoch 481/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0077 - lr: 0.1000\n",
      "Epoch 482/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0077 - lr: 0.1000\n",
      "Epoch 483/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0076 - lr: 0.1000\n",
      "Epoch 484/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0076 - lr: 0.1000\n",
      "Epoch 485/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0076 - lr: 0.1000\n",
      "Epoch 486/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0076 - lr: 0.1000\n",
      "Epoch 487/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0076 - lr: 0.1000\n",
      "Epoch 488/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0075 - lr: 0.1000\n",
      "Epoch 489/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0075 - lr: 0.1000\n",
      "Epoch 490/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0075 - lr: 0.1000\n",
      "Epoch 491/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075 - lr: 0.1000\n",
      "Epoch 492/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075 - lr: 0.1000\n",
      "Epoch 493/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075 - lr: 0.0100\n",
      "Epoch 494/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075 - lr: 0.0100\n",
      "Epoch 495/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075 - lr: 0.0100\n",
      "Epoch 496/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0075 - lr: 0.0100\n",
      "Epoch 497/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 0.0100\n",
      "Epoch 498/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 0.0100\n",
      "Epoch 499/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-03\n",
      "Epoch 500/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-03\n",
      "Epoch 501/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-03\n",
      "Epoch 502/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-03\n",
      "Epoch 503/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-03\n",
      "Epoch 504/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 505/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 506/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 507/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 508/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-04\n",
      "Epoch 509/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-05\n",
      "Epoch 510/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-05\n",
      "Epoch 511/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-05\n",
      "Epoch 512/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-05\n",
      "Epoch 513/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-05\n",
      "Epoch 514/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-06\n",
      "Epoch 515/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-06\n",
      "Epoch 516/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-06\n",
      "Epoch 517/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-06\n",
      "Epoch 518/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-06\n",
      "Epoch 519/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-07\n",
      "Epoch 520/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-07\n",
      "Epoch 521/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-07\n",
      "Epoch 522/2000\n",
      "347/347 [==============================] - 1s 4ms/step - loss: 0.0073 - val_loss: 0.0074 - lr: 1.0000e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x72b5e45f2f90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train_lstm,\n",
    "    y_train_lstm,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "#model.save_weights(f'saved_models/{folder}/{model_name}/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4227e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f'saved_models/{folder}/{model_name}/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7ad3281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, None, 5)]         0         \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, None, 5)           795       \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, None, 40)          7360      \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, None, 5)           605       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8760 (34.22 KB)\n",
      "Trainable params: 7965 (31.11 KB)\n",
      "Non-trainable params: 795 (3.11 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#transfer learning\n",
    "#the whole model\n",
    "model.trainable = False\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "x = model(input_layer, training=False)\n",
    "x = layers.LSTM(40, return_sequences=True)(x)\n",
    "\n",
    "filters = input_shape[-1]\n",
    "output_layer = layers.Conv1D(\n",
    "    filters=filters,\n",
    "    kernel_size=3,\n",
    "    activation=\"sigmoid\",\n",
    "    padding=\"same\",\n",
    "    data_format='channels_last'\n",
    ")(x)\n",
    "\n",
    "model_new = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84240305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.rnn.lstm.LSTM at 0x76f05f660290>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ce3b6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, None, 5)]         0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, None, 10)          640       \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, None, 10)          840       \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, None, 5)           155       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1635 (6.39 KB)\n",
      "Trainable params: 995 (3.89 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#transfer learning\n",
    "#the whole model\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "trained_layer = model.layers[1](input_layer)\n",
    "trained_layer.trainable = False\n",
    "new_lstm_layer = layers.LSTM(10, return_sequences=True, activation=\"relu\")(trained_layer)\n",
    "\n",
    "filters = input_shape[-1]\n",
    "output_layer = layers.Conv1D(\n",
    "    filters=filters,\n",
    "    kernel_size=3,\n",
    "    activation=\"sigmoid\",\n",
    "    padding=\"same\",\n",
    "    data_format='channels_last'\n",
    ")(new_lstm_layer)\n",
    "\n",
    "model_new = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48cf8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'mean_squared_error'\n",
    "learning_rate = 0.1\n",
    "optimizer_name = \"Adadelta\"\n",
    "optimizer = tf.keras.optimizers.legacy.Adadelta(learning_rate=learning_rate, name=optimizer_name)\n",
    "\n",
    "model_new.compile(\n",
    "    loss=loss, \n",
    "    optimizer=optimizer \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03987aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training 16 ifDense False bp \n",
    "\n",
    "folder = 'lstm'\n",
    "epochs = 2000\n",
    "batch_size = 2 \n",
    "validation_split = 0.1\n",
    "\n",
    "model_name = f'model_lstm_tl2_units_{unit_numb}_bs2_{input_shape[-1]}_ifDense_{ifDense}'\n",
    "\n",
    "#define callbacks\n",
    "# Write TensorBoard logs\n",
    "log_dir = f'./tensorboard/{folder}/logs/{model_name}'\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "#Stop training when no improvement\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                  patience=10, \n",
    "                                                  restore_best_weights=True)\n",
    "#Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=5)\n",
    "callbacks = [tensorboard, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd339dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "347/347 [==============================] - 14s 37ms/step - loss: 0.0429 - val_loss: 0.0402 - lr: 0.1000\n",
      "Epoch 2/2000\n",
      "347/347 [==============================] - 13s 36ms/step - loss: 0.0405 - val_loss: 0.0379 - lr: 0.1000\n",
      "Epoch 3/2000\n",
      "231/347 [==================>...........] - ETA: 4s - loss: 0.0392"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_new\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      2\u001b[0m     x_train_lstm,\n\u001b[1;32m      3\u001b[0m     y_train_lstm,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m      6\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39mvalidation_split,\n\u001b[1;32m      7\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    870\u001b[0m   )\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1492\u001b[0m   )\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_new.fit(\n",
    "    x_train_lstm,\n",
    "    y_train_lstm,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d93d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "878bc5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 85576), started 0:22:32 ago. (Use '!kill 85576' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ce9ad64c12bc860\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ce9ad64c12bc860\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tensorboard results\n",
    "folder = 'lstm'\n",
    "\n",
    "model_name = f'model_lstm_tl2_units_{unit_numb}_bs2_{input_shape[-1]}_ifDense_{ifDense}'\n",
    "\n",
    "log_dir = f'./tensorboard/{folder}/logs/{model_name}'\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af24b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-951061fd4af6103f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-951061fd4af6103f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tensorboard results\n",
    "folder = 'lstm'\n",
    "cells_list = [10]\n",
    "\n",
    "model_name = f'model_lstm_units_{unit_numb}_bs2_{input_shape[-1]}_{cells_list_str}_ifDense_{ifDense}'\n",
    "\n",
    "log_dir = f'./tensorboard/{folder}/logs/{model_name}'\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ae1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
