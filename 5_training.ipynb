{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d5bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc20c9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 15:30:46.160995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 15:30:46.161025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 15:30:46.161830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 15:30:46.166330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 15:30:46.718995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from build_model_ed import build_model_1, build_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb38ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#gpu check\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b3cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (22400, 2048, 50, 3)\n",
      "Training dataset dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "test_size = 0.1\n",
    "stride_step = 50\n",
    "norm_factor=1\n",
    "dtype = 'uint8'\n",
    "\n",
    "filename = f'save_data//e_d_train_all_str_{stride_step}_ts_{test_size}_{dtype}_norm_{norm_factor}.pkl'\n",
    "filename = f'save_data//e_d_train_Puck_str_{stride_step}_ts_{test_size}_{dtype}_norm_{norm_factor}.pkl'\n",
    "#load train data\n",
    "with open(filename, 'rb') as f:\n",
    "    train_list,frames_data_train = pickle.load(f)\n",
    "    \n",
    "print(f'Training dataset shape: {frames_data_train.shape}')\n",
    "print(f'Training dataset dtype: {frames_data_train.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adec9eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 15:42:21.842806: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 15:42:22.775106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46708 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-02-27 15:42:22.775699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46686 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:73:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.convert_to_tensor(frames_data_train)\n",
    "y_train = tf.convert_to_tensor(frames_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9b334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 15:53:21.920525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 15:53:22.873039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46708 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-02-27 15:53:22.873612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46686 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:73:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "\n",
    "x_train = tf.convert_to_tensor(frames_data_train[:index])\n",
    "y_train = tf.convert_to_tensor(frames_data_train[:index])\n",
    "\n",
    "#frames_data_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c096b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_data_train = frames_data_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d37332",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'model_encoder_decoder_test'\n",
    "\n",
    "dense_units = 100\n",
    "ifBatchNorm = True\n",
    "kernel_size = (3,3)\n",
    "nn_blocks = [\n",
    "    [16, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [32, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [64, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [128, kernel_size, ifBatchNorm, (4,1)]\n",
    "]\n",
    "\n",
    "if 1:\n",
    "    #save data\n",
    "    save_data = [dense_units,ifBatchNorm,kernel_size,nn_blocks]\n",
    "    file_name = f'saved_models//{model_name}_settings.pkl'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(save_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b200c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'model_encoder_decoder_test_2'\n",
    "\n",
    "dense_units = 100\n",
    "ifBatchNorm = True\n",
    "kernel_size = (3,3)\n",
    "nn_blocks = [\n",
    "    [16, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [32, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [64, kernel_size, ifBatchNorm, (2,1)],\n",
    "    [128, kernel_size, ifBatchNorm, (2,1)]\n",
    "]\n",
    "\n",
    "if 1:\n",
    "    #save data\n",
    "    save_data = [dense_units,ifBatchNorm,kernel_size,nn_blocks]\n",
    "    file_name = f'saved_models//{model_name}_settings.pkl'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(save_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7a965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'model_encoder_decoder_test_3'\n",
    "\n",
    "dense_units = 200\n",
    "ifBatchNorm = True\n",
    "kernel_size = (3,3)\n",
    "nn_blocks = [\n",
    "    [16, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [32, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [64, kernel_size, ifBatchNorm, (4,1)],\n",
    "    [128, kernel_size, ifBatchNorm, (4,1)]\n",
    "]\n",
    "\n",
    "if 1:\n",
    "    #save data\n",
    "    save_data = [dense_units,ifBatchNorm,kernel_size,nn_blocks]\n",
    "    file_name = f'saved_models//{model_name}_settings.pkl'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(save_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cccbca8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2048, 50, 3)\n",
      "Last conv layer shape: (None, 8, 50, 128)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2048, 50, 3)]     0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 2048, 50, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 2048, 50, 16)      448       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 2048, 50, 16)      64        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 512, 50, 16)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 512, 50, 32)       4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512, 50, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 128, 50, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 50, 64)       18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128, 50, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 32, 50, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 50, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 32, 50, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 50, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 51200)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               10240200  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 51200)             10291200  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 50, 128)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 50, 128)        147584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 8, 50, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 32, 50, 128)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 32, 50, 64)        73792     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 32, 50, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 128, 50, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 128, 50, 32)       18464     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 128, 50, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSamplin  (None, 512, 50, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 512, 50, 16)       4624      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512, 50, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSamplin  (None, 2048, 50, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2048, 50, 3)       435       \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 2048, 50, 3)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20875659 (79.63 MB)\n",
      "Trainable params: 20874699 (79.63 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data_shape = (61600, 2048, 50, 3)\n",
    "\n",
    "input_shape = data_shape[1:]\n",
    "\n",
    "print(f'Input shape: {input_shape}')\n",
    "\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.1, name=\"Adadelta\")\n",
    "#optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.01, name=\"SGD\")\n",
    "\n",
    "\n",
    "model = build_model_2(input_shape, dense_units, nn_blocks)\n",
    "\n",
    "model.compile(loss=loss, \n",
    "              optimizer=optimizer, )\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49947547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "epochs = 2000\n",
    "batch_size = 3 \n",
    "validation_split = 0.2\n",
    "\n",
    "#define callbacks\n",
    "# Write TensorBoard logs\n",
    "log_dir = f'./tensorboard/logs/{model_name}'\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "#Stop training when no improvement\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                  patience=10, \n",
    "                                                  restore_best_weights=True)\n",
    "#Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=5)\n",
    "callbacks = [tensorboard, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9eacff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "if 0:\n",
    "    ld = log_dir\n",
    "else:\n",
    "    ld=None\n",
    "!rm -rf $ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4a77f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709399665.599891    4370 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5974/5974 [==============================] - 90s 14ms/step - loss: 591.4893 - val_loss: 818.0576 - lr: 0.1000\n",
      "Epoch 2/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 140.8608 - val_loss: 243.1705 - lr: 0.1000\n",
      "Epoch 3/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 122.5822 - val_loss: 194.5189 - lr: 0.1000\n",
      "Epoch 4/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 112.9621 - val_loss: 173.2908 - lr: 0.1000\n",
      "Epoch 5/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 106.8634 - val_loss: 197.2255 - lr: 0.1000\n",
      "Epoch 6/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 101.9698 - val_loss: 164.6332 - lr: 0.1000\n",
      "Epoch 7/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 98.9356 - val_loss: 223.0548 - lr: 0.1000\n",
      "Epoch 8/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 95.4113 - val_loss: 135.7680 - lr: 0.1000\n",
      "Epoch 9/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 93.0581 - val_loss: 134.2357 - lr: 0.1000\n",
      "Epoch 10/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 90.9551 - val_loss: 130.5475 - lr: 0.1000\n",
      "Epoch 11/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 88.7776 - val_loss: 154.2027 - lr: 0.1000\n",
      "Epoch 12/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 87.0323 - val_loss: 136.0898 - lr: 0.1000\n",
      "Epoch 13/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 85.8756 - val_loss: 123.6586 - lr: 0.1000\n",
      "Epoch 14/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 84.7463 - val_loss: 126.7017 - lr: 0.1000\n",
      "Epoch 15/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 83.1007 - val_loss: 126.9854 - lr: 0.1000\n",
      "Epoch 16/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 82.5306 - val_loss: 115.3134 - lr: 0.1000\n",
      "Epoch 17/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 81.1891 - val_loss: 122.8090 - lr: 0.1000\n",
      "Epoch 18/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 80.7194 - val_loss: 117.7813 - lr: 0.1000\n",
      "Epoch 19/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 79.5406 - val_loss: 121.6090 - lr: 0.1000\n",
      "Epoch 20/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 78.8177 - val_loss: 112.2017 - lr: 0.1000\n",
      "Epoch 21/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 78.0099 - val_loss: 117.2943 - lr: 0.1000\n",
      "Epoch 22/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 77.3150 - val_loss: 115.8701 - lr: 0.1000\n",
      "Epoch 23/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 76.3179 - val_loss: 116.0969 - lr: 0.1000\n",
      "Epoch 24/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 75.9072 - val_loss: 136.3182 - lr: 0.1000\n",
      "Epoch 25/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 75.4899 - val_loss: 112.9715 - lr: 0.1000\n",
      "Epoch 26/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 74.7911 - val_loss: 105.5722 - lr: 0.1000\n",
      "Epoch 27/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 74.2029 - val_loss: 122.9818 - lr: 0.1000\n",
      "Epoch 28/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 73.7710 - val_loss: 114.6639 - lr: 0.1000\n",
      "Epoch 29/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 72.8292 - val_loss: 103.7466 - lr: 0.1000\n",
      "Epoch 30/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 72.5500 - val_loss: 107.3170 - lr: 0.1000\n",
      "Epoch 31/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 72.6068 - val_loss: 108.5689 - lr: 0.1000\n",
      "Epoch 32/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 71.7653 - val_loss: 108.0192 - lr: 0.1000\n",
      "Epoch 33/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 71.4073 - val_loss: 103.4881 - lr: 0.1000\n",
      "Epoch 34/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 71.1426 - val_loss: 99.9641 - lr: 0.1000\n",
      "Epoch 35/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 70.7785 - val_loss: 104.2169 - lr: 0.1000\n",
      "Epoch 36/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 70.5310 - val_loss: 106.0310 - lr: 0.1000\n",
      "Epoch 37/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 70.3165 - val_loss: 116.7106 - lr: 0.1000\n",
      "Epoch 38/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 69.7381 - val_loss: 103.9316 - lr: 0.1000\n",
      "Epoch 39/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 69.5176 - val_loss: 102.8303 - lr: 0.1000\n",
      "Epoch 40/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 68.9940 - val_loss: 106.6029 - lr: 0.1000\n",
      "Epoch 41/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 69.0008 - val_loss: 106.8161 - lr: 0.1000\n",
      "Epoch 42/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 68.7518 - val_loss: 101.1019 - lr: 0.1000\n",
      "Epoch 43/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 68.1719 - val_loss: 99.7224 - lr: 0.1000\n",
      "Epoch 44/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 67.9905 - val_loss: 100.0785 - lr: 0.1000\n",
      "Epoch 45/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 67.8384 - val_loss: 100.8352 - lr: 0.1000\n",
      "Epoch 46/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 67.3027 - val_loss: 105.3648 - lr: 0.1000\n",
      "Epoch 47/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 67.3095 - val_loss: 102.8503 - lr: 0.1000\n",
      "Epoch 48/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 67.1799 - val_loss: 115.2545 - lr: 0.1000\n",
      "Epoch 49/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 66.8555 - val_loss: 99.5012 - lr: 0.1000\n",
      "Epoch 50/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 66.3961 - val_loss: 103.0878 - lr: 0.1000\n",
      "Epoch 51/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 66.3722 - val_loss: 95.9321 - lr: 0.1000\n",
      "Epoch 52/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 65.9390 - val_loss: 104.2298 - lr: 0.1000\n",
      "Epoch 53/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 65.8913 - val_loss: 99.7495 - lr: 0.1000\n",
      "Epoch 54/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 65.7672 - val_loss: 102.0693 - lr: 0.1000\n",
      "Epoch 55/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 65.4085 - val_loss: 98.6087 - lr: 0.1000\n",
      "Epoch 56/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 65.2049 - val_loss: 95.5170 - lr: 0.1000\n",
      "Epoch 57/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 65.0736 - val_loss: 105.4678 - lr: 0.1000\n",
      "Epoch 58/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 64.8167 - val_loss: 103.0902 - lr: 0.1000\n",
      "Epoch 59/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 64.8354 - val_loss: 97.4999 - lr: 0.1000\n",
      "Epoch 60/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 64.6560 - val_loss: 100.2662 - lr: 0.1000\n",
      "Epoch 61/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 64.2800 - val_loss: 102.3042 - lr: 0.1000\n",
      "Epoch 62/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 64.1290 - val_loss: 97.6412 - lr: 0.1000\n",
      "Epoch 63/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 64.0737 - val_loss: 98.3023 - lr: 0.1000\n",
      "Epoch 64/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 63.7469 - val_loss: 95.1742 - lr: 0.1000\n",
      "Epoch 65/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 63.5652 - val_loss: 96.9377 - lr: 0.1000\n",
      "Epoch 66/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 63.5376 - val_loss: 101.8999 - lr: 0.1000\n",
      "Epoch 67/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 63.2738 - val_loss: 105.8201 - lr: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 63.1861 - val_loss: 94.2092 - lr: 0.1000\n",
      "Epoch 69/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 63.1987 - val_loss: 105.9021 - lr: 0.1000\n",
      "Epoch 70/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 62.8857 - val_loss: 99.6161 - lr: 0.1000\n",
      "Epoch 71/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 62.8777 - val_loss: 98.5971 - lr: 0.1000\n",
      "Epoch 72/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 62.6949 - val_loss: 107.7446 - lr: 0.1000\n",
      "Epoch 73/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 62.5917 - val_loss: 95.3765 - lr: 0.1000\n",
      "Epoch 74/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 62.4527 - val_loss: 95.4678 - lr: 0.1000\n",
      "Epoch 75/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 62.1010 - val_loss: 95.5188 - lr: 0.1000\n",
      "Epoch 76/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 62.0836 - val_loss: 97.8438 - lr: 0.1000\n",
      "Epoch 77/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 61.9941 - val_loss: 106.9264 - lr: 0.1000\n",
      "Epoch 78/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.9420 - val_loss: 96.1135 - lr: 0.1000\n",
      "Epoch 79/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.6611 - val_loss: 102.1408 - lr: 0.1000\n",
      "Epoch 80/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.4878 - val_loss: 94.8393 - lr: 0.1000\n",
      "Epoch 81/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.3784 - val_loss: 100.8542 - lr: 0.1000\n",
      "Epoch 82/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 61.4817 - val_loss: 98.0172 - lr: 0.1000\n",
      "Epoch 83/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.3378 - val_loss: 98.3642 - lr: 0.1000\n",
      "Epoch 84/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.0218 - val_loss: 94.5695 - lr: 0.1000\n",
      "Epoch 85/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 61.0139 - val_loss: 95.8913 - lr: 0.1000\n",
      "Epoch 86/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.7629 - val_loss: 91.9183 - lr: 0.1000\n",
      "Epoch 87/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.6049 - val_loss: 99.5086 - lr: 0.1000\n",
      "Epoch 88/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.6109 - val_loss: 101.2319 - lr: 0.1000\n",
      "Epoch 89/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.4810 - val_loss: 93.8182 - lr: 0.1000\n",
      "Epoch 90/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.5657 - val_loss: 93.4620 - lr: 0.1000\n",
      "Epoch 91/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.3229 - val_loss: 96.3254 - lr: 0.1000\n",
      "Epoch 92/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.3655 - val_loss: 94.6460 - lr: 0.1000\n",
      "Epoch 93/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.0214 - val_loss: 97.3430 - lr: 0.1000\n",
      "Epoch 94/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 60.0490 - val_loss: 94.7287 - lr: 0.1000\n",
      "Epoch 95/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.8741 - val_loss: 96.0070 - lr: 0.1000\n",
      "Epoch 96/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.9109 - val_loss: 94.7512 - lr: 0.1000\n",
      "Epoch 97/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 59.8291 - val_loss: 92.1007 - lr: 0.1000\n",
      "Epoch 98/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.8549 - val_loss: 92.7005 - lr: 0.1000\n",
      "Epoch 99/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.5172 - val_loss: 94.4971 - lr: 0.1000\n",
      "Epoch 100/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.3183 - val_loss: 92.4892 - lr: 0.1000\n",
      "Epoch 101/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.1770 - val_loss: 90.4039 - lr: 0.1000\n",
      "Epoch 102/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.9929 - val_loss: 94.0330 - lr: 0.1000\n",
      "Epoch 103/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 59.0873 - val_loss: 93.4203 - lr: 0.1000\n",
      "Epoch 104/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 58.9614 - val_loss: 93.9790 - lr: 0.1000\n",
      "Epoch 105/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 58.9084 - val_loss: 110.4333 - lr: 0.1000\n",
      "Epoch 106/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 58.6205 - val_loss: 89.6472 - lr: 0.1000\n",
      "Epoch 107/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 58.5544 - val_loss: 99.1273 - lr: 0.1000\n",
      "Epoch 108/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.6076 - val_loss: 90.7210 - lr: 0.1000\n",
      "Epoch 109/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.5190 - val_loss: 89.4902 - lr: 0.1000\n",
      "Epoch 110/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 58.3752 - val_loss: 92.3271 - lr: 0.1000\n",
      "Epoch 111/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.4671 - val_loss: 93.8874 - lr: 0.1000\n",
      "Epoch 112/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.2331 - val_loss: 92.5089 - lr: 0.1000\n",
      "Epoch 113/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.1837 - val_loss: 92.3210 - lr: 0.1000\n",
      "Epoch 114/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.0651 - val_loss: 94.4334 - lr: 0.1000\n",
      "Epoch 115/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 58.0862 - val_loss: 96.2594 - lr: 0.1000\n",
      "Epoch 116/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.9516 - val_loss: 89.7188 - lr: 0.1000\n",
      "Epoch 117/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.8556 - val_loss: 92.1952 - lr: 0.1000\n",
      "Epoch 118/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.6273 - val_loss: 90.2147 - lr: 0.1000\n",
      "Epoch 119/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.6932 - val_loss: 90.3076 - lr: 0.1000\n",
      "Epoch 120/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.5110 - val_loss: 91.8227 - lr: 0.1000\n",
      "Epoch 121/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.3428 - val_loss: 91.2427 - lr: 0.1000\n",
      "Epoch 122/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.2580 - val_loss: 89.4095 - lr: 0.1000\n",
      "Epoch 123/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 57.3881 - val_loss: 95.8614 - lr: 0.1000\n",
      "Epoch 124/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 57.1316 - val_loss: 90.4431 - lr: 0.1000\n",
      "Epoch 125/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 57.1592 - val_loss: 92.1700 - lr: 0.1000\n",
      "Epoch 126/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 57.0676 - val_loss: 88.0280 - lr: 0.1000\n",
      "Epoch 127/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 57.0686 - val_loss: 92.5707 - lr: 0.1000\n",
      "Epoch 128/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 56.9784 - val_loss: 89.9500 - lr: 0.1000\n",
      "Epoch 129/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 57.0030 - val_loss: 94.5008 - lr: 0.1000\n",
      "Epoch 130/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.7804 - val_loss: 91.1815 - lr: 0.1000\n",
      "Epoch 131/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.7353 - val_loss: 90.4422 - lr: 0.1000\n",
      "Epoch 132/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.6122 - val_loss: 90.0295 - lr: 0.1000\n",
      "Epoch 133/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 56.5197 - val_loss: 93.8327 - lr: 0.1000\n",
      "Epoch 134/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 56.5121 - val_loss: 102.2404 - lr: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 56.5791 - val_loss: 90.1940 - lr: 0.1000\n",
      "Epoch 136/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 56.3235 - val_loss: 89.2889 - lr: 0.1000\n",
      "Epoch 137/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.2888 - val_loss: 92.3005 - lr: 0.1000\n",
      "Epoch 138/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.3482 - val_loss: 91.8940 - lr: 0.1000\n",
      "Epoch 139/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.1289 - val_loss: 91.0469 - lr: 0.1000\n",
      "Epoch 140/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.1117 - val_loss: 89.4775 - lr: 0.1000\n",
      "Epoch 141/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 56.0438 - val_loss: 90.8094 - lr: 0.1000\n",
      "Epoch 142/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.8598 - val_loss: 87.9959 - lr: 0.1000\n",
      "Epoch 143/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.8699 - val_loss: 88.5508 - lr: 0.1000\n",
      "Epoch 144/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.8480 - val_loss: 90.2080 - lr: 0.1000\n",
      "Epoch 145/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.7076 - val_loss: 86.1649 - lr: 0.1000\n",
      "Epoch 146/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 55.6827 - val_loss: 90.4087 - lr: 0.1000\n",
      "Epoch 147/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 55.5931 - val_loss: 88.2432 - lr: 0.1000\n",
      "Epoch 148/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 55.6336 - val_loss: 87.7601 - lr: 0.1000\n",
      "Epoch 149/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 55.4128 - val_loss: 90.2804 - lr: 0.1000\n",
      "Epoch 150/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.4727 - val_loss: 89.6614 - lr: 0.1000\n",
      "Epoch 151/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.3493 - val_loss: 87.7397 - lr: 0.1000\n",
      "Epoch 152/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.3329 - val_loss: 89.9209 - lr: 0.1000\n",
      "Epoch 153/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.2075 - val_loss: 87.1623 - lr: 0.1000\n",
      "Epoch 154/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.1534 - val_loss: 88.0195 - lr: 0.1000\n",
      "Epoch 155/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.9894 - val_loss: 101.1944 - lr: 0.1000\n",
      "Epoch 156/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.1240 - val_loss: 86.1338 - lr: 0.1000\n",
      "Epoch 157/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.9620 - val_loss: 89.1162 - lr: 0.1000\n",
      "Epoch 158/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 55.0491 - val_loss: 89.2481 - lr: 0.1000\n",
      "Epoch 159/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.7581 - val_loss: 90.7291 - lr: 0.1000\n",
      "Epoch 160/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.9906 - val_loss: 91.9560 - lr: 0.1000\n",
      "Epoch 161/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.7764 - val_loss: 88.0503 - lr: 0.1000\n",
      "Epoch 162/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.8393 - val_loss: 98.5976 - lr: 0.1000\n",
      "Epoch 163/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.6772 - val_loss: 87.8842 - lr: 0.1000\n",
      "Epoch 164/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.5804 - val_loss: 90.6989 - lr: 0.1000\n",
      "Epoch 165/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 54.5980 - val_loss: 92.6684 - lr: 0.1000\n",
      "Epoch 166/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 54.4446 - val_loss: 87.9023 - lr: 0.1000\n",
      "Epoch 167/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 54.5241 - val_loss: 87.5223 - lr: 0.1000\n",
      "Epoch 168/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 54.3214 - val_loss: 89.2674 - lr: 0.1000\n",
      "Epoch 169/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 54.1609 - val_loss: 98.6292 - lr: 0.1000\n",
      "Epoch 170/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 54.2350 - val_loss: 87.9683 - lr: 0.1000\n",
      "Epoch 171/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 54.1921 - val_loss: 87.5236 - lr: 0.1000\n",
      "Epoch 172/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.1829 - val_loss: 87.9605 - lr: 0.1000\n",
      "Epoch 173/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.0080 - val_loss: 88.2375 - lr: 0.1000\n",
      "Epoch 174/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.9698 - val_loss: 86.3695 - lr: 0.1000\n",
      "Epoch 175/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 54.0412 - val_loss: 87.5925 - lr: 0.1000\n",
      "Epoch 176/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.8916 - val_loss: 86.6017 - lr: 0.1000\n",
      "Epoch 177/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.8572 - val_loss: 90.2623 - lr: 0.1000\n",
      "Epoch 178/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.8123 - val_loss: 87.4869 - lr: 0.1000\n",
      "Epoch 179/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.7468 - val_loss: 88.8151 - lr: 0.1000\n",
      "Epoch 180/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.7946 - val_loss: 88.7245 - lr: 0.1000\n",
      "Epoch 181/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 53.7725 - val_loss: 88.2394 - lr: 0.1000\n",
      "Epoch 182/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.5525 - val_loss: 91.6379 - lr: 0.1000\n",
      "Epoch 183/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.6410 - val_loss: 88.2918 - lr: 0.1000\n",
      "Epoch 184/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.4770 - val_loss: 86.2098 - lr: 0.1000\n",
      "Epoch 185/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.5036 - val_loss: 86.7296 - lr: 0.1000\n",
      "Epoch 186/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.3922 - val_loss: 88.3663 - lr: 0.1000\n",
      "Epoch 187/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.4490 - val_loss: 86.3168 - lr: 0.1000\n",
      "Epoch 188/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.2183 - val_loss: 90.5417 - lr: 0.1000\n",
      "Epoch 189/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.3289 - val_loss: 86.9794 - lr: 0.1000\n",
      "Epoch 190/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.2386 - val_loss: 87.1608 - lr: 0.1000\n",
      "Epoch 191/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.1910 - val_loss: 89.2764 - lr: 0.1000\n",
      "Epoch 192/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 53.1000 - val_loss: 86.4132 - lr: 0.1000\n",
      "Epoch 193/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 53.0755 - val_loss: 86.7915 - lr: 0.1000\n",
      "Epoch 194/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.9554 - val_loss: 85.5627 - lr: 0.1000\n",
      "Epoch 195/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.8828 - val_loss: 87.7531 - lr: 0.1000\n",
      "Epoch 196/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.8847 - val_loss: 84.8899 - lr: 0.1000\n",
      "Epoch 197/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.8554 - val_loss: 87.0353 - lr: 0.1000\n",
      "Epoch 198/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.8075 - val_loss: 87.5369 - lr: 0.1000\n",
      "Epoch 199/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.7059 - val_loss: 87.8607 - lr: 0.1000\n",
      "Epoch 200/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 52.7194 - val_loss: 85.7965 - lr: 0.1000\n",
      "Epoch 201/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.7034 - val_loss: 89.8266 - lr: 0.1000\n",
      "Epoch 202/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.6273 - val_loss: 84.6905 - lr: 0.1000\n",
      "Epoch 203/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.5897 - val_loss: 85.4242 - lr: 0.1000\n",
      "Epoch 204/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.5549 - val_loss: 93.0930 - lr: 0.1000\n",
      "Epoch 205/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.5607 - val_loss: 86.1462 - lr: 0.1000\n",
      "Epoch 206/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 52.4517 - val_loss: 86.7243 - lr: 0.1000\n",
      "Epoch 207/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.4300 - val_loss: 85.3640 - lr: 0.1000\n",
      "Epoch 208/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.4179 - val_loss: 84.8479 - lr: 0.1000\n",
      "Epoch 209/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.2769 - val_loss: 86.4669 - lr: 0.1000\n",
      "Epoch 210/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.2483 - val_loss: 84.6135 - lr: 0.1000\n",
      "Epoch 211/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.2709 - val_loss: 85.6741 - lr: 0.1000\n",
      "Epoch 212/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.1642 - val_loss: 86.9916 - lr: 0.1000\n",
      "Epoch 213/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.1991 - val_loss: 86.5218 - lr: 0.1000\n",
      "Epoch 214/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 52.1682 - val_loss: 94.1519 - lr: 0.1000\n",
      "Epoch 215/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.0519 - val_loss: 87.6105 - lr: 0.1000\n",
      "Epoch 216/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.9977 - val_loss: 85.2034 - lr: 0.1000\n",
      "Epoch 217/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.9939 - val_loss: 90.3522 - lr: 0.1000\n",
      "Epoch 218/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 52.0114 - val_loss: 84.2139 - lr: 0.1000\n",
      "Epoch 219/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.9320 - val_loss: 86.7135 - lr: 0.1000\n",
      "Epoch 220/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.8528 - val_loss: 85.5645 - lr: 0.1000\n",
      "Epoch 221/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.8953 - val_loss: 89.4292 - lr: 0.1000\n",
      "Epoch 222/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 51.7611 - val_loss: 84.6985 - lr: 0.1000\n",
      "Epoch 223/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.8576 - val_loss: 87.3656 - lr: 0.1000\n",
      "Epoch 224/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.7644 - val_loss: 87.4942 - lr: 0.1000\n",
      "Epoch 225/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.6809 - val_loss: 85.3249 - lr: 0.1000\n",
      "Epoch 226/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.6448 - val_loss: 86.4975 - lr: 0.1000\n",
      "Epoch 227/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.6108 - val_loss: 86.0693 - lr: 0.1000\n",
      "Epoch 228/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.5061 - val_loss: 86.0830 - lr: 0.1000\n",
      "Epoch 229/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.5099 - val_loss: 86.1690 - lr: 0.1000\n",
      "Epoch 230/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.5026 - val_loss: 84.5278 - lr: 0.1000\n",
      "Epoch 231/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.4570 - val_loss: 86.5326 - lr: 0.1000\n",
      "Epoch 232/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.4977 - val_loss: 86.6131 - lr: 0.1000\n",
      "Epoch 233/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.3669 - val_loss: 85.7522 - lr: 0.1000\n",
      "Epoch 234/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.3969 - val_loss: 89.4462 - lr: 0.1000\n",
      "Epoch 235/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.2554 - val_loss: 86.7396 - lr: 0.1000\n",
      "Epoch 236/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.2240 - val_loss: 84.0017 - lr: 0.1000\n",
      "Epoch 237/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.2489 - val_loss: 86.4209 - lr: 0.1000\n",
      "Epoch 238/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.1643 - val_loss: 92.3593 - lr: 0.1000\n",
      "Epoch 239/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.1447 - val_loss: 84.0585 - lr: 0.1000\n",
      "Epoch 240/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.0570 - val_loss: 88.4083 - lr: 0.1000\n",
      "Epoch 241/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.1363 - val_loss: 89.3657 - lr: 0.1000\n",
      "Epoch 242/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 51.0728 - val_loss: 86.8439 - lr: 0.1000\n",
      "Epoch 243/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.9287 - val_loss: 84.5055 - lr: 0.1000\n",
      "Epoch 244/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.9506 - val_loss: 85.7879 - lr: 0.1000\n",
      "Epoch 245/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.9762 - val_loss: 86.2775 - lr: 0.1000\n",
      "Epoch 246/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.9197 - val_loss: 84.0991 - lr: 0.1000\n",
      "Epoch 247/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.9239 - val_loss: 85.0660 - lr: 0.1000\n",
      "Epoch 248/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.9135 - val_loss: 84.5205 - lr: 0.1000\n",
      "Epoch 249/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 50.7588 - val_loss: 85.8148 - lr: 0.1000\n",
      "Epoch 250/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.6974 - val_loss: 86.1386 - lr: 0.1000\n",
      "Epoch 251/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.6965 - val_loss: 84.8355 - lr: 0.1000\n",
      "Epoch 252/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 50.7013 - val_loss: 84.0778 - lr: 0.1000\n",
      "Epoch 253/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.6718 - val_loss: 83.9895 - lr: 0.1000\n",
      "Epoch 254/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.5793 - val_loss: 85.2157 - lr: 0.1000\n",
      "Epoch 255/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.5928 - val_loss: 84.9945 - lr: 0.1000\n",
      "Epoch 256/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 50.5628 - val_loss: 85.0038 - lr: 0.1000\n",
      "Epoch 257/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 50.5093 - val_loss: 84.3626 - lr: 0.1000\n",
      "Epoch 258/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 50.4812 - val_loss: 83.0745 - lr: 0.1000\n",
      "Epoch 259/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.3743 - val_loss: 90.6714 - lr: 0.1000\n",
      "Epoch 260/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.3716 - val_loss: 85.1206 - lr: 0.1000\n",
      "Epoch 261/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.3669 - val_loss: 86.2869 - lr: 0.1000\n",
      "Epoch 262/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.3520 - val_loss: 84.9070 - lr: 0.1000\n",
      "Epoch 263/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.2935 - val_loss: 83.7737 - lr: 0.1000\n",
      "Epoch 264/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.2807 - val_loss: 86.6618 - lr: 0.1000\n",
      "Epoch 265/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.4033 - val_loss: 87.5656 - lr: 0.1000\n",
      "Epoch 266/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.1410 - val_loss: 83.3111 - lr: 0.1000\n",
      "Epoch 267/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 50.1447 - val_loss: 83.4350 - lr: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.1844 - val_loss: 84.1742 - lr: 0.1000\n",
      "Epoch 269/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.0975 - val_loss: 85.5257 - lr: 0.1000\n",
      "Epoch 270/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.1028 - val_loss: 85.9165 - lr: 0.1000\n",
      "Epoch 271/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.1217 - val_loss: 86.7600 - lr: 0.1000\n",
      "Epoch 272/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 50.0258 - val_loss: 81.9005 - lr: 0.1000\n",
      "Epoch 273/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 50.0157 - val_loss: 85.6674 - lr: 0.1000\n",
      "Epoch 274/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.9280 - val_loss: 83.0580 - lr: 0.1000\n",
      "Epoch 275/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.9088 - val_loss: 83.9057 - lr: 0.1000\n",
      "Epoch 276/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.9376 - val_loss: 83.6405 - lr: 0.1000\n",
      "Epoch 277/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.7967 - val_loss: 82.3823 - lr: 0.1000\n",
      "Epoch 278/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.8460 - val_loss: 85.7148 - lr: 0.1000\n",
      "Epoch 279/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 49.8858 - val_loss: 83.6727 - lr: 0.1000\n",
      "Epoch 280/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.8108 - val_loss: 82.7478 - lr: 0.1000\n",
      "Epoch 281/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.7841 - val_loss: 82.9730 - lr: 0.1000\n",
      "Epoch 282/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.8054 - val_loss: 83.7710 - lr: 0.1000\n",
      "Epoch 283/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.6426 - val_loss: 83.3313 - lr: 0.1000\n",
      "Epoch 284/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.7288 - val_loss: 83.2183 - lr: 0.1000\n",
      "Epoch 285/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.6604 - val_loss: 85.3671 - lr: 0.1000\n",
      "Epoch 286/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.6488 - val_loss: 83.5399 - lr: 0.1000\n",
      "Epoch 287/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.5414 - val_loss: 82.2714 - lr: 0.1000\n",
      "Epoch 288/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.6054 - val_loss: 83.6157 - lr: 0.1000\n",
      "Epoch 289/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.5162 - val_loss: 81.6719 - lr: 0.1000\n",
      "Epoch 290/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.4384 - val_loss: 84.1792 - lr: 0.1000\n",
      "Epoch 291/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.4695 - val_loss: 84.0095 - lr: 0.1000\n",
      "Epoch 292/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.5055 - val_loss: 84.1144 - lr: 0.1000\n",
      "Epoch 293/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.3714 - val_loss: 83.6140 - lr: 0.1000\n",
      "Epoch 294/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.3577 - val_loss: 87.5284 - lr: 0.1000\n",
      "Epoch 295/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.4319 - val_loss: 90.5156 - lr: 0.1000\n",
      "Epoch 296/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 49.3036 - val_loss: 82.8692 - lr: 0.1000\n",
      "Epoch 297/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.3026 - val_loss: 82.6981 - lr: 0.1000\n",
      "Epoch 298/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.2356 - val_loss: 86.4141 - lr: 0.1000\n",
      "Epoch 299/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.2650 - val_loss: 82.9171 - lr: 0.1000\n",
      "Epoch 300/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.2669 - val_loss: 85.0000 - lr: 0.1000\n",
      "Epoch 301/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.1749 - val_loss: 82.6648 - lr: 0.1000\n",
      "Epoch 302/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.1740 - val_loss: 81.4382 - lr: 0.1000\n",
      "Epoch 303/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.1513 - val_loss: 82.0918 - lr: 0.1000\n",
      "Epoch 304/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.1187 - val_loss: 81.5490 - lr: 0.1000\n",
      "Epoch 305/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.1514 - val_loss: 81.7021 - lr: 0.1000\n",
      "Epoch 306/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 49.0726 - val_loss: 84.5425 - lr: 0.1000\n",
      "Epoch 307/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 49.0193 - val_loss: 81.4587 - lr: 0.1000\n",
      "Epoch 308/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 49.0165 - val_loss: 82.5523 - lr: 0.1000\n",
      "Epoch 309/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 48.9802 - val_loss: 81.1376 - lr: 0.1000\n",
      "Epoch 310/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 49.0019 - val_loss: 80.9030 - lr: 0.1000\n",
      "Epoch 311/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.9502 - val_loss: 80.6784 - lr: 0.1000\n",
      "Epoch 312/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.9597 - val_loss: 80.6918 - lr: 0.1000\n",
      "Epoch 313/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.9451 - val_loss: 81.9911 - lr: 0.1000\n",
      "Epoch 314/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.9632 - val_loss: 81.8055 - lr: 0.1000\n",
      "Epoch 315/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 48.8464 - val_loss: 80.8862 - lr: 0.1000\n",
      "Epoch 316/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.8668 - val_loss: 83.5344 - lr: 0.1000\n",
      "Epoch 317/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.8369 - val_loss: 83.6385 - lr: 0.1000\n",
      "Epoch 318/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.7477 - val_loss: 83.8307 - lr: 0.1000\n",
      "Epoch 319/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.7855 - val_loss: 80.9781 - lr: 0.1000\n",
      "Epoch 320/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 48.7735 - val_loss: 81.5955 - lr: 0.1000\n",
      "Epoch 321/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.7473 - val_loss: 79.9646 - lr: 0.1000\n",
      "Epoch 322/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.7329 - val_loss: 82.9428 - lr: 0.1000\n",
      "Epoch 323/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.7280 - val_loss: 83.6482 - lr: 0.1000\n",
      "Epoch 324/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 48.6181 - val_loss: 80.6525 - lr: 0.1000\n",
      "Epoch 325/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.6214 - val_loss: 80.5296 - lr: 0.1000\n",
      "Epoch 326/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.5650 - val_loss: 80.1901 - lr: 0.1000\n",
      "Epoch 327/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.5747 - val_loss: 81.9035 - lr: 0.1000\n",
      "Epoch 328/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.6363 - val_loss: 83.7307 - lr: 0.1000\n",
      "Epoch 329/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.5608 - val_loss: 84.7823 - lr: 0.1000\n",
      "Epoch 330/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.5549 - val_loss: 80.2107 - lr: 0.1000\n",
      "Epoch 331/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.6002 - val_loss: 80.5896 - lr: 0.1000\n",
      "Epoch 332/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.5013 - val_loss: 79.9147 - lr: 0.1000\n",
      "Epoch 333/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.4944 - val_loss: 83.7084 - lr: 0.1000\n",
      "Epoch 334/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.4674 - val_loss: 79.9296 - lr: 0.1000\n",
      "Epoch 335/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.4287 - val_loss: 80.6017 - lr: 0.1000\n",
      "Epoch 336/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.4122 - val_loss: 79.4927 - lr: 0.1000\n",
      "Epoch 337/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.4147 - val_loss: 79.8868 - lr: 0.1000\n",
      "Epoch 338/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.4617 - val_loss: 83.7398 - lr: 0.1000\n",
      "Epoch 339/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.3559 - val_loss: 80.5651 - lr: 0.1000\n",
      "Epoch 340/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.3293 - val_loss: 80.1576 - lr: 0.1000\n",
      "Epoch 341/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.3144 - val_loss: 79.4168 - lr: 0.1000\n",
      "Epoch 342/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.3155 - val_loss: 80.2949 - lr: 0.1000\n",
      "Epoch 343/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.3046 - val_loss: 79.8897 - lr: 0.1000\n",
      "Epoch 344/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.2732 - val_loss: 79.4141 - lr: 0.1000\n",
      "Epoch 345/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.2530 - val_loss: 79.3729 - lr: 0.1000\n",
      "Epoch 346/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.2167 - val_loss: 79.0270 - lr: 0.1000\n",
      "Epoch 347/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.2467 - val_loss: 79.8921 - lr: 0.1000\n",
      "Epoch 348/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.1842 - val_loss: 82.5177 - lr: 0.1000\n",
      "Epoch 349/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.1290 - val_loss: 79.9826 - lr: 0.1000\n",
      "Epoch 350/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.1529 - val_loss: 79.7981 - lr: 0.1000\n",
      "Epoch 351/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.1293 - val_loss: 79.8955 - lr: 0.1000\n",
      "Epoch 352/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 48.1450 - val_loss: 79.1590 - lr: 0.1000\n",
      "Epoch 353/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 48.1353 - val_loss: 80.3236 - lr: 0.1000\n",
      "Epoch 354/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 48.0112 - val_loss: 79.7860 - lr: 0.1000\n",
      "Epoch 355/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 48.0405 - val_loss: 79.0119 - lr: 0.1000\n",
      "Epoch 356/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.0857 - val_loss: 79.0226 - lr: 0.1000\n",
      "Epoch 357/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.9664 - val_loss: 79.8244 - lr: 0.1000\n",
      "Epoch 358/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 48.0575 - val_loss: 80.9953 - lr: 0.1000\n",
      "Epoch 359/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.9701 - val_loss: 78.9986 - lr: 0.1000\n",
      "Epoch 360/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.9873 - val_loss: 79.3739 - lr: 0.1000\n",
      "Epoch 361/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.9583 - val_loss: 82.2786 - lr: 0.1000\n",
      "Epoch 362/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.9543 - val_loss: 78.8657 - lr: 0.1000\n",
      "Epoch 363/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 47.9637 - val_loss: 80.0797 - lr: 0.1000\n",
      "Epoch 364/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.9223 - val_loss: 80.0695 - lr: 0.1000\n",
      "Epoch 365/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.8369 - val_loss: 81.3787 - lr: 0.1000\n",
      "Epoch 366/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.8363 - val_loss: 80.7707 - lr: 0.1000\n",
      "Epoch 367/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.8665 - val_loss: 83.0330 - lr: 0.1000\n",
      "Epoch 368/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.8593 - val_loss: 89.7208 - lr: 0.1000\n",
      "Epoch 369/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.8710 - val_loss: 79.4326 - lr: 0.1000\n",
      "Epoch 370/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.7982 - val_loss: 80.0588 - lr: 0.1000\n",
      "Epoch 371/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.7897 - val_loss: 78.2901 - lr: 0.1000\n",
      "Epoch 372/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.7401 - val_loss: 77.7969 - lr: 0.1000\n",
      "Epoch 373/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 47.7583 - val_loss: 79.8730 - lr: 0.1000\n",
      "Epoch 374/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.7245 - val_loss: 79.2020 - lr: 0.1000\n",
      "Epoch 375/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.7182 - val_loss: 78.7017 - lr: 0.1000\n",
      "Epoch 376/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.7572 - val_loss: 83.0506 - lr: 0.1000\n",
      "Epoch 377/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6831 - val_loss: 79.6766 - lr: 0.1000\n",
      "Epoch 378/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6788 - val_loss: 78.7869 - lr: 0.1000\n",
      "Epoch 379/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6289 - val_loss: 85.1007 - lr: 0.1000\n",
      "Epoch 380/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6408 - val_loss: 79.2821 - lr: 0.1000\n",
      "Epoch 381/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6306 - val_loss: 81.3681 - lr: 0.1000\n",
      "Epoch 382/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5506 - val_loss: 79.0893 - lr: 0.1000\n",
      "Epoch 383/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6453 - val_loss: 78.9718 - lr: 0.1000\n",
      "Epoch 384/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.6385 - val_loss: 80.6556 - lr: 0.1000\n",
      "Epoch 385/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5446 - val_loss: 78.7885 - lr: 0.1000\n",
      "Epoch 386/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5644 - val_loss: 78.6333 - lr: 0.1000\n",
      "Epoch 387/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5672 - val_loss: 78.9329 - lr: 0.1000\n",
      "Epoch 388/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5432 - val_loss: 78.9137 - lr: 0.1000\n",
      "Epoch 389/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5259 - val_loss: 77.6877 - lr: 0.1000\n",
      "Epoch 390/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5010 - val_loss: 78.4784 - lr: 0.1000\n",
      "Epoch 391/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.5288 - val_loss: 78.5711 - lr: 0.1000\n",
      "Epoch 392/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 47.4645 - val_loss: 78.5852 - lr: 0.1000\n",
      "Epoch 393/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.4518 - val_loss: 82.9568 - lr: 0.1000\n",
      "Epoch 394/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.4619 - val_loss: 79.0669 - lr: 0.1000\n",
      "Epoch 395/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.4703 - val_loss: 86.3193 - lr: 0.1000\n",
      "Epoch 396/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.4158 - val_loss: 77.9110 - lr: 0.1000\n",
      "Epoch 397/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.4103 - val_loss: 81.8511 - lr: 0.1000\n",
      "Epoch 398/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 47.4105 - val_loss: 83.3841 - lr: 0.1000\n",
      "Epoch 399/2000\n",
      "5974/5974 [==============================] - 79s 13ms/step - loss: 47.3315 - val_loss: 80.5923 - lr: 0.1000\n",
      "Epoch 400/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 47.3876 - val_loss: 77.4140 - lr: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 47.3378 - val_loss: 77.9979 - lr: 0.1000\n",
      "Epoch 402/2000\n",
      "5974/5974 [==============================] - 78s 13ms/step - loss: 47.3193 - val_loss: 80.2456 - lr: 0.1000\n",
      "Epoch 403/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.3018 - val_loss: 77.4843 - lr: 0.1000\n",
      "Epoch 404/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.3724 - val_loss: 77.6337 - lr: 0.1000\n",
      "Epoch 405/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.2584 - val_loss: 78.9759 - lr: 0.1000\n",
      "Epoch 406/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.3077 - val_loss: 78.9004 - lr: 0.1000\n",
      "Epoch 407/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.3083 - val_loss: 78.8062 - lr: 0.1000\n",
      "Epoch 408/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 47.3014 - val_loss: 79.6658 - lr: 0.1000\n",
      "Epoch 409/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.2996 - val_loss: 79.3734 - lr: 0.1000\n",
      "Epoch 410/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.2075 - val_loss: 80.3554 - lr: 0.1000\n",
      "Epoch 411/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.2051 - val_loss: 78.5988 - lr: 0.1000\n",
      "Epoch 412/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.2167 - val_loss: 77.4470 - lr: 0.1000\n",
      "Epoch 413/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1938 - val_loss: 78.1512 - lr: 0.1000\n",
      "Epoch 414/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.2007 - val_loss: 81.9891 - lr: 0.1000\n",
      "Epoch 415/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1589 - val_loss: 77.8658 - lr: 0.1000\n",
      "Epoch 416/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1599 - val_loss: 81.6491 - lr: 0.1000\n",
      "Epoch 417/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1400 - val_loss: 77.7998 - lr: 0.1000\n",
      "Epoch 418/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1018 - val_loss: 77.4074 - lr: 0.1000\n",
      "Epoch 419/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1056 - val_loss: 77.1932 - lr: 0.1000\n",
      "Epoch 420/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.0850 - val_loss: 78.7970 - lr: 0.1000\n",
      "Epoch 421/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.1210 - val_loss: 76.8018 - lr: 0.1000\n",
      "Epoch 422/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 47.1062 - val_loss: 77.6429 - lr: 0.1000\n",
      "Epoch 423/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 47.1014 - val_loss: 76.7489 - lr: 0.1000\n",
      "Epoch 424/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.0513 - val_loss: 80.1441 - lr: 0.1000\n",
      "Epoch 425/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9994 - val_loss: 78.1897 - lr: 0.1000\n",
      "Epoch 426/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.0108 - val_loss: 77.2007 - lr: 0.1000\n",
      "Epoch 427/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.0300 - val_loss: 79.3543 - lr: 0.1000\n",
      "Epoch 428/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 47.0451 - val_loss: 77.8949 - lr: 0.1000\n",
      "Epoch 429/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9989 - val_loss: 77.2454 - lr: 0.1000\n",
      "Epoch 430/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9595 - val_loss: 77.1338 - lr: 0.1000\n",
      "Epoch 431/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9615 - val_loss: 77.5018 - lr: 0.1000\n",
      "Epoch 432/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9631 - val_loss: 78.8345 - lr: 0.1000\n",
      "Epoch 433/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9159 - val_loss: 87.9235 - lr: 0.1000\n",
      "Epoch 434/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 46.9340 - val_loss: 80.8803 - lr: 0.1000\n",
      "Epoch 435/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9045 - val_loss: 77.3162 - lr: 0.1000\n",
      "Epoch 436/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9120 - val_loss: 80.0402 - lr: 0.1000\n",
      "Epoch 437/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9689 - val_loss: 77.4654 - lr: 0.1000\n",
      "Epoch 438/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.9336 - val_loss: 77.7028 - lr: 0.1000\n",
      "Epoch 439/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8882 - val_loss: 78.3450 - lr: 0.1000\n",
      "Epoch 440/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8560 - val_loss: 83.0817 - lr: 0.1000\n",
      "Epoch 441/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8382 - val_loss: 82.0208 - lr: 0.1000\n",
      "Epoch 442/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8234 - val_loss: 77.8370 - lr: 0.1000\n",
      "Epoch 443/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8302 - val_loss: 78.2425 - lr: 0.1000\n",
      "Epoch 444/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7997 - val_loss: 76.9706 - lr: 0.1000\n",
      "Epoch 445/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8754 - val_loss: 79.5777 - lr: 0.1000\n",
      "Epoch 446/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7964 - val_loss: 78.4377 - lr: 0.1000\n",
      "Epoch 447/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7641 - val_loss: 81.4664 - lr: 0.1000\n",
      "Epoch 448/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7686 - val_loss: 78.6343 - lr: 0.1000\n",
      "Epoch 449/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 46.7777 - val_loss: 77.7344 - lr: 0.1000\n",
      "Epoch 450/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.8127 - val_loss: 77.1446 - lr: 0.1000\n",
      "Epoch 451/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.6852 - val_loss: 78.0928 - lr: 0.1000\n",
      "Epoch 452/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7646 - val_loss: 77.2633 - lr: 0.1000\n",
      "Epoch 453/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7126 - val_loss: 76.4763 - lr: 0.1000\n",
      "Epoch 454/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7038 - val_loss: 79.5987 - lr: 0.1000\n",
      "Epoch 455/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 46.7291 - val_loss: 78.4019 - lr: 0.1000\n",
      "Epoch 456/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 46.7161 - val_loss: 84.2024 - lr: 0.1000\n",
      "Epoch 457/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.4332 - val_loss: 75.1169 - lr: 0.0100\n",
      "Epoch 458/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.3051 - val_loss: 75.1075 - lr: 0.0100\n",
      "Epoch 459/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.2471 - val_loss: 75.0442 - lr: 0.0100\n",
      "Epoch 460/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1908 - val_loss: 74.9432 - lr: 0.0100\n",
      "Epoch 461/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1614 - val_loss: 74.9591 - lr: 0.0100\n",
      "Epoch 462/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1783 - val_loss: 74.9566 - lr: 0.0100\n",
      "Epoch 463/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1534 - val_loss: 75.0591 - lr: 0.0100\n",
      "Epoch 464/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1592 - val_loss: 75.1595 - lr: 0.0100\n",
      "Epoch 465/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1036 - val_loss: 74.9089 - lr: 0.0100\n",
      "Epoch 466/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1402 - val_loss: 74.8860 - lr: 0.0100\n",
      "Epoch 467/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0797 - val_loss: 74.8967 - lr: 0.0100\n",
      "Epoch 468/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0984 - val_loss: 74.9094 - lr: 0.0100\n",
      "Epoch 469/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1096 - val_loss: 74.7133 - lr: 0.0100\n",
      "Epoch 470/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0685 - val_loss: 74.8769 - lr: 0.0100\n",
      "Epoch 471/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0658 - val_loss: 74.8707 - lr: 0.0100\n",
      "Epoch 472/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1026 - val_loss: 75.0370 - lr: 0.0100\n",
      "Epoch 473/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0207 - val_loss: 74.9159 - lr: 0.0100\n",
      "Epoch 474/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1080 - val_loss: 74.8762 - lr: 0.0100\n",
      "Epoch 475/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0477 - val_loss: 75.0297 - lr: 0.0100\n",
      "Epoch 476/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.1110 - val_loss: 74.8806 - lr: 0.0100\n",
      "Epoch 477/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0452 - val_loss: 75.0472 - lr: 0.0100\n",
      "Epoch 478/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 45.0678 - val_loss: 74.8277 - lr: 0.0100\n",
      "Epoch 479/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9930 - val_loss: 74.8031 - lr: 1.0000e-03\n",
      "Epoch 480/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9953 - val_loss: 74.9197 - lr: 1.0000e-03\n",
      "Epoch 481/2000\n",
      "5974/5974 [==============================] - 76s 13ms/step - loss: 44.9978 - val_loss: 74.7794 - lr: 1.0000e-03\n",
      "Epoch 482/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9321 - val_loss: 74.8263 - lr: 1.0000e-03\n",
      "Epoch 483/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9551 - val_loss: 74.8079 - lr: 1.0000e-03\n",
      "Epoch 484/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9697 - val_loss: 74.8123 - lr: 1.0000e-03\n",
      "Epoch 485/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9981 - val_loss: 74.8209 - lr: 1.0000e-03\n",
      "Epoch 486/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9373 - val_loss: 74.8460 - lr: 1.0000e-03\n",
      "Epoch 487/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9851 - val_loss: 74.8083 - lr: 1.0000e-03\n",
      "Epoch 488/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9643 - val_loss: 74.8791 - lr: 1.0000e-04\n",
      "Epoch 489/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9866 - val_loss: 74.7310 - lr: 1.0000e-04\n",
      "Epoch 490/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9490 - val_loss: 74.8544 - lr: 1.0000e-04\n",
      "Epoch 491/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9490 - val_loss: 74.8550 - lr: 1.0000e-04\n",
      "Epoch 492/2000\n",
      "5974/5974 [==============================] - 77s 13ms/step - loss: 44.9720 - val_loss: 74.8243 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2499af3310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=frames_data_train,\n",
    "    y=frames_data_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c50f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_weights(f'saved_models/{model_name}/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13aa3e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-37b1ce0511b6b92f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-37b1ce0511b6b92f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tensorboard results\n",
    "\n",
    "model_name = f'model_encoder_decoder_test_3'\n",
    "\n",
    "log_dir = f'./tensorboard/logs/{model_name}'\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5ba3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
